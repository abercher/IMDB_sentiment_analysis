{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB dataset: simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will try to implement some basic models not relying on neural networks to perform sentiment analysis on the IMDB data set. This will give a baseline to measure the performance of my more advanced models.\n",
    "This script has been stronlgy inspired by the \"nlp.ipynb\" notebook from the fastai dl1 course (17.06.18 version).\n",
    "\n",
    "This notebook was finished on the 19.06.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is located on my personal machine but is also available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'\n",
    "TRAIN_ALL = PATH+'train/all/'\n",
    "TEST_ALL = PATH+'test/all/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function was taken from the text.py file from fastai (17.06.18)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_labels_from_folders(path, folders):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(folders):\n",
    "        for fname in glob(os.path.join(path, label, '*.*')):\n",
    "            texts.append(open(fname, 'r').read())\n",
    "            labels.append(idx)\n",
    "    return texts, np.array(labels).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r in X_trn_r stands for raw, as the text is still not preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['neg','pos']\n",
    "X_trn_r,y_trn = texts_labels_from_folders(TRAIN,names)\n",
    "X_val_r,y_val = texts_labels_from_folders(TEST,names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a BOW representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW with only frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a BOW representation of the data. In this case every column displays how many times a word appears in the given text. And we are using 3-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the size of the vocabulary. I took it from the fastai notebook. I have no idea how it is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next variable represents the minimum frequency of a word among the different documents. Given the fact that we have 25K files, I guess than 0.1% should be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next variable represents if the column for a token only represent the presence (1) or absence (0) of a word (i.e. BINARY = True) or if it counts the number of times a word appears in the given text (i.e. BINARY = FALSE). According to this paper (Section 2.1):\n",
    "https://www.aclweb.org/anthology/P12-2018\n",
    "binarizing gives better results for Naive Bayes estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the tool to transform each text into a long vector which entries indicate how many times the corresponding words from the vocabulary (vocabulary which is also built during the fitting process) has been found in the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr_freq = CountVectorizer(ngram_range=(1,3), min_df=MIN_FREQ, max_features=VOCAB_SIZE, binary=BINARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the training set is transformed into vectors and the vocabulary is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn_freq = veczr_freq.fit_transform(X_trn_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the validation set is transformed into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_freq = veczr_freq.transform(X_val_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 45467)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_freq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_freq = veczr_freq.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW with binary entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a BOW representation of the data, where every column indicates the presence or absence of a word/token/n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr_bin = CountVectorizer(ngram_range=(1,3), min_df=MIN_FREQ, max_features=VOCAB_SIZE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn_bin = veczr_bin.fit_transform(X_trn_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_bin = veczr_bin.transform(X_val_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_bin = veczr_bin.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW with tf-idf coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a BOW representation of the data, where every column indicates the tf-idf score of the word/token/n-gram for the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn_tfidf = tfidf_transformer.fit_transform(X_trn_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tfidf = tfidf_transformer.transform(X_val_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn_all = [X_trn_freq, X_trn_bin, X_trn_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_all = [X_val_freq, X_val_bin, X_val_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['BOW frequency', 'BOW binary', 'BOW tf-idf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [multi_nb, logreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = ['Multinomial Naive Bayes', 'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X=X_trn_freq, y=y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = logreg.predict(X=X_val_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = logreg.predict_proba(X=X_val_freq)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the performances of our different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_true, y_pred, y_prob):\n",
    "    print('accuracy = ', accuracy_score(y_true=y_true, y_pred=y_pred))\n",
    "    print('F1 score binary = ', f1_score(y_true=y_true, y_pred=y_pred, average='binary'))\n",
    "    print('Recall score = ', recall_score(y_true=y_true, y_pred=y_pred, average='macro'))\n",
    "    print('Average precision score = ', average_precision_score(y_true=y_true, y_score=y_prob))\n",
    "    print('Air under the ROC = ', roc_auc_score(y_true=y_true, y_score=y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW frequency\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "accuracy =  0.8608\n",
      "F1 score binary =  0.85989210081327\n",
      "Recall score =  0.8608\n",
      "Average precision score =  0.9079606563446728\n",
      "Air under the ROC =  0.9265069152\n",
      "**********************\n",
      "Logistic Regression\n",
      "accuracy =  0.88716\n",
      "F1 score binary =  0.8874346594309883\n",
      "Recall score =  0.88716\n",
      "Average precision score =  0.9500510501470008\n",
      "Air under the ROC =  0.9531011584\n",
      "**********************\n",
      "######################\n",
      "BOW binary\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "accuracy =  0.87164\n",
      "F1 score binary =  0.8716656668666266\n",
      "Recall score =  0.87164\n",
      "Average precision score =  0.927364166914979\n",
      "Air under the ROC =  0.9388091423999999\n",
      "**********************\n",
      "Logistic Regression\n",
      "accuracy =  0.89084\n",
      "F1 score binary =  0.8912185594132419\n",
      "Recall score =  0.8908400000000001\n",
      "Average precision score =  0.9520401370998973\n",
      "Air under the ROC =  0.9550804736\n",
      "**********************\n",
      "######################\n",
      "BOW tf-idf\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "accuracy =  0.87396\n",
      "F1 score binary =  0.8741060369970833\n",
      "Recall score =  0.8739600000000001\n",
      "Average precision score =  0.9433772099774644\n",
      "Air under the ROC =  0.946014144\n",
      "**********************\n",
      "Logistic Regression\n",
      "accuracy =  0.89568\n",
      "F1 score binary =  0.8962196577795464\n",
      "Recall score =  0.89568\n",
      "Average precision score =  0.9589803412241802\n",
      "Air under the ROC =  0.9605377216000001\n",
      "**********************\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_names)):\n",
    "    print(data_names[i])\n",
    "    print('')\n",
    "    X_trn = X_trn_all[i]\n",
    "    X_val = X_val_all[i]\n",
    "    for j in range(len(models_names)):\n",
    "        print(models_names[j])\n",
    "        clf = models[j]\n",
    "        clf.fit(X=X_trn, y=y_trn)\n",
    "        y_pred = clf.predict(X=X_val)\n",
    "        y_prob = clf.predict_proba(X=X_val)[:, 1]\n",
    "        print_evaluation_scores(y_val, y_pred, y_prob)\n",
    "        print('**********************')\n",
    "    print('######################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
