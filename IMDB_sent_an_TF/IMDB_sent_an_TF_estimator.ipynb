{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB reviews: TensorFlow GloVe and LSTM with Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to perform sentiment analysis using TensorFlow. Most of the notebook is a variation of what was done on this blog:\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "This is an upgrade of the previous notebook (IMDB_sent_an_TF_basic_improved1) where I'm replacing the basic APIs by custom Estimator level APIs. For this I follow the indications of these tutorial:\n",
    "\n",
    "https://www.tensorflow.org/get_started/premade_estimators\n",
    "\n",
    "https://www.tensorflow.org/get_started/datasets_quickstart\n",
    "\n",
    "https://www.tensorflow.org/get_started/custom_estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "In this notebook, I use the smallest ones, i.e. the ones where the word embedding vectors are of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_file_name = 'glove.6B/glove.6B.50d_toy.txt' #toy embeddings with only the\n",
    "                                                #three first rows (instead of 4K)\n",
    "emb_file_name = 'glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the next cell comes from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "emb_df = pd.read_table(emb_file_name, sep=\" \",\n",
    "                       index_col=0, header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the Pandas data frame into one list containing the words (the indexes from the data frame) and one numpy array containing the corresponding vectors. This last data frame will play the role of our **look-up table** later when we define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = list(emb_df.index)\n",
    "wordVectors = emb_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I commented this part because it is useless for the real task. It only serves a pedagogic purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseballIndex = wordsList.index('baseball')\n",
    "#wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxSeqLength = 10 #Maximum length of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "#firstSentence[0] = wordsList.index(\"i\")\n",
    "#firstSentence[1] = wordsList.index(\"thought\")\n",
    "#firstSentence[2] = wordsList.index(\"the\")\n",
    "#firstSentence[3] = wordsList.index(\"movie\")\n",
    "#firstSentence[4] = wordsList.index(\"was\")\n",
    "#firstSentence[5] = wordsList.index(\"incredible\")\n",
    "#firstSentence[6] = wordsList.index(\"and\")\n",
    "#firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "##firstSentence[8] and firstSentence[9] are going to be 0\n",
    "#print(firstSentence.shape)\n",
    "#print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are looking for the threshold we should take as maximum length of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_POS = TRAIN + 'pos/'\n",
    "TRAIN_NEG = TRAIN + 'neg/'\n",
    "positiveFilesTrain = [TRAIN_POS + f for f in listdir(TRAIN_POS) if isfile(join(TRAIN_POS, f))]\n",
    "negativeFilesTrain = [TRAIN_NEG + f for f in listdir(TRAIN_NEG) if isfile(join(TRAIN_NEG, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_POS = TEST + 'pos/'\n",
    "TEST_NEG = TEST + 'neg/'\n",
    "positiveFilesTest = [TEST_POS + f for f in listdir(TEST_POS) if isfile(join(TEST_POS, f))]\n",
    "negativeFilesTest = [TEST_NEG + f for f in listdir(TEST_NEG) if isfile(join(TEST_NEG, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine the average number of words in one sample of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "numWords = []\n",
    "for pf in positiveFilesTrain:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFilesTrain:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFilesTrain = len(positiveFilesTrain) + len(negativeFilesTrain)\n",
    "numFilesTest = len(positiveFilesTest) + len(negativeFilesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely it seems that there aren't exactly 12500 files in the folders indicated below, as it is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n",
      "12501\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveFilesTrain))\n",
    "print(len(negativeFilesTrain))\n",
    "print(len(positiveFilesTest))\n",
    "print(len(negativeFilesTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 25002\n",
      "The total number of words in the files is 5844682\n",
      "The average number of words in the files is 233.7685785137189\n"
     ]
    }
   ],
   "source": [
    "print('The total number of files is', numFilesTrain)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an histogram of the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, 250 words seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we map words to indexes using our wordsList variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do it for a specific file (I commented this part because it is useless for the real task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = positiveFilesTrain[3] #Can use any valid index (not just 3)\n",
    "#with open(fname) as f:\n",
    "#    for lines in f:\n",
    "#        print(lines)\n",
    "#        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "#with open(fname) as f:\n",
    "#    indexCounter = 0\n",
    "#    line=f.readline()\n",
    "#    cleanedLine = cleanSentences(line)\n",
    "#    split = cleanedLine.split()\n",
    "#    for word in split:\n",
    "#        try:\n",
    "#            firstFile[indexCounter] = wordsList.index(word)\n",
    "#        except ValueError:\n",
    "#            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "#        indexCounter = indexCounter + 1\n",
    "#firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids_train will be a matrix, which contains for each file of the training set (first the positive ones, then the negative ones) a row where the columns contain the indices corresponding to the words of the sample file.\n",
    "\n",
    "The whole computation of the transformation of each text file into a list of indices takes time and needs to be performed only once. The result is saved after the first time and then reloaded for all the other executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_train = np.zeros((numFilesTrain, maxSeqLength), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCounter = 0\n",
    "#for pf in positiveFilesTrain:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFilesTrain:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "#np.save('idsMatrixTrain', ids_train)\n",
    "\n",
    "ids_train = np.load('idsMatrixTrain.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some toy data to be able to perform quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train_toy = ids_train[12400:12600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_test = np.zeros((numFilesTest, maxSeqLength), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCounter = 0\n",
    "#for pf in positiveFilesTest:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFilesTest:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('idsMatrixTest', ids_test)\n",
    "\n",
    "ids_test = np.load('idsMatrixTest.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also create the **labels** with **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative), as done in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTrain = len(positiveFilesTrain)\n",
    "lenNegTrain = len(negativeFilesTrain)\n",
    "y_train = [[1,0] for i in range(lenPosTrain)] + [[0,1] for i in range(lenNegTrain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTest = len(positiveFilesTest)\n",
    "lenNegTest = len(negativeFilesTest)\n",
    "y_test = [[1,0] for i in range(lenPosTest)] + [[0,1] for i in range(lenNegTest)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the labels for the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_toy = y_train[12400:12600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some premade estimators do not accept one-hot-encoding of the labels as explained here:https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "\n",
    "For this reason I create a function which converts a list containing one-hot-incoded labels into a list containing **ordinal encoded** labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot2Ordinal(oneHot):\n",
    "    n_classes = len(oneHot[0])\n",
    "    ordinal = []\n",
    "    for i in range(len(oneHot)):\n",
    "        for j in range(n_classes):\n",
    "            if oneHot[i][j]==1:\n",
    "                ordinal.append(j)\n",
    "    return(ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_train_toy_ord = oneHot2Ordinal(y_train_toy)\n",
    "print(len(y_train_toy_ord))\n",
    "print(y_train_toy_ord[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ord = oneHot2Ordinal(y_train)\n",
    "y_test_ord = oneHot2Ordinal(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a model using the Estimator APIs from TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the **input functions**. They are the objects which supply data for training, evaluating, and prediction to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'Indexes':features}, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = {'Indexes':features}\n",
    "    else:\n",
    "        inputs = ({'Indexes':features}, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **feature columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = []\n",
    "my_feature_columns.append(tf.feature_column.numeric_column(key='Indexes', shape=maxSeqLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the directory where to store the log files for TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define a custom estimator we need to define a **model function**. For this we mix the code of the notebook based only on basic TF APIs together with some parts of the script of this tutorial: https://www.tensorflow.org/get_started/custom_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    # Use `input_layer` to apply the feature columns.\n",
    "    input_data = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # The next line is required because tf.feature_column.input_layer\n",
    "    # outputs tf.float32 (whatever the input)\n",
    "    # and tf.nn.embedding_lookup requires\n",
    "    # tf.int32\n",
    "    input_data = tf.cast(input_data, tf.int32)\n",
    "    # Transform each index in a sentence into the associated vector\n",
    "    data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "    # The following line is a fixe coming from this page:\n",
    "    # https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/issues/2\n",
    "    # in order to prevent an error appearing next.\n",
    "    data = tf.cast(data, tf.float32)\n",
    "    # Next we define the LSTM\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(params['lstmUnits'])\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=params['keep_prob'])\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "    # swaps the two first dimensions so it has dimensions [max_time, batch_size, cell.output_size]\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    # If I'm not mistaken the next cell slices the part of \n",
    "    # the output which corresponds to the last output of the lstm, \n",
    "    # or in other words the output corresponding to the \n",
    "    # last word for every sample (if I'm right we used \n",
    "    # 0 padding and cut everything which goes beyound 250 words, \n",
    "    # so technically it is the 250th output). \n",
    "    # My guess is that last has dimensions [batch_size, cell.output_size] \n",
    "    # which we can then use to do matrix multiplication \n",
    "    # with weight which has dimensions [cell.output_size, numClasses] \n",
    "    # (remember that cell.output_size=lstmUnits).\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    # We apply an affine transformation to get the logits\n",
    "    weight = tf.Variable(tf.truncated_normal([params['lstmUnits'], params['n_classes']]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[params['n_classes']]))\n",
    "    logits = (tf.matmul(last, weight) + bias)\n",
    "    # Maybe I could  replace this last part using tf.layers.dense:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "    \n",
    "    # The following lines are actually independent of the achitecture\n",
    "    # of the model.\n",
    "    \n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "# TensorBoard 1 (method original Notebook)\n",
    "    \n",
    "#    correctPred = tf.equal(tf.argmax(predicted_classes,1), tf.argmax(labels,1))\n",
    "#    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "#    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_classes, labels=labels))\n",
    "#    tf.summary.scalar('Loss', loss)\n",
    "#    tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "#TensorBoard 2 (method TF tutorial)\n",
    "    \n",
    "    # Compute loss\n",
    "    \n",
    "    # Note that because of this function, we have to\n",
    "    # provide ordinaly encoded labels and not one-hot-encoded\n",
    "    # labels, as explained on this page:\n",
    "    # https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define the **custom estimator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_is_chief': True, '_log_step_count_steps': 100, '_task_type': 'worker', '_service': None, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_master': '', '_task_id': 0, '_evaluation_master': '', '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_save_summary_steps': 100, '_model_dir': '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/', '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5e5f36ada0>, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        model_dir=model_dir,\n",
    "        params={\n",
    "            'feature_columns': my_feature_columns,\n",
    "            'n_classes': 2,\n",
    "            'lstmUnits': 64,\n",
    "            'keep_prob': 0.8\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation of the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(32)\n",
    "train_steps = int(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can **train** our Estimator. Note that for the premade estimator that we are using requires the label to be ordinaly encoded and not one-hot-encoded as explained here:\n",
    "\n",
    "https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/aritz/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.749625, step = 0\n",
      "INFO:tensorflow:global_step/sec: 6.945\n",
      "INFO:tensorflow:loss = 0.0, step = 100 (14.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.53953\n",
      "INFO:tensorflow:loss = 0.0, step = 200 (13.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.55014\n",
      "INFO:tensorflow:loss = 0.0, step = 300 (13.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.48705\n",
      "INFO:tensorflow:loss = 0.65994954, step = 400 (13.356 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.005850534.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f5e5f36a630>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(features=ids_train, labels=y_train_ord, batch_size=batch_size),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-08-20:05:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-08-20:06:10\n",
      "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.49998, global_step = 500, loss = 2.6552541\n"
     ]
    }
   ],
   "source": [
    "evaluation_test = classifier.evaluate(input_fn=lambda:eval_input_fn(features=ids_test,\n",
    "                                                                    labels=y_test_ord,\n",
    "                                                                    batch_size=batch_size)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author of the tutorial also mentions the possibility of tracking the progress of the model on TensorBoard by entering \"tensorboard --logdir=(...)\" in a terminal with \"(...)\" replaced by the name of the directory where the event files are saved, and visiting http://localhost:6006/ with a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we present the original implementation of the model, relying on TF basic APIs. It will be replaced by estimators APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These placeholders are here to take the input of the model (labels and samples turned into arrays of indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we embed the indices into vectors. The next cell is commented out because I think that it is useless. It was in the tutorial but I suspect that its author forgot to remove it.\n",
    "As explained in the tutorial, we were using pretrained embeddings where vectors have length 50. But here numDimensions are of length 300. And in the following cell, 'data' defined again... I ran the notebook with and without it and it gives similar results so I commented it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a fix comming from\n",
    "https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/issues/2\n",
    "If I don't put it, errors appear in the cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.cast(data, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the LSTM with dropout layer. According to the tutorial, the parameter lstmUnits needs some tuning to find the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I understood it right, 'value' in the next cell represents the outputs of the lstm (for each sample of the batch and each word of each sample). According to the documentation it should have dimensions equal to [batch_size, max_time, cell.output_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add some afine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm not mistaken the next cell swaps the two first dimensions so it has dimensions [max_time, batch_size, cell.output_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf.transpose(value, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm not mistaken the next cell slices the part of the output which corresponds to the last output of the lstm, or in other words the output corresponding to the last word for every sample (if I'm right we used 0 padding and cut everything which goes beyound 250 words, so technically it is the 250th output). My guess is that last has dimensions [batch_size, cell.output_size] which we can then use to do matrix multiplication with weight which has dimensions [cell.output_size, numClasses] (remember that cell.output_size=lstmUnits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the cross-entropy loss using the logits (i.e. unnormalized probabilities), and we define the optimizer. Note that I replaced tf.nn.softmax_cross_entropy_with_logits (as in the original script) by tf.nn.softmax_cross_entropy_with_logits_v2 as indicated by a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows us to use TensorBoard to visualize the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterations gives the number of batches against whom we are going to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the author of the tutorial I'm copying, the training takes a lot of time. For this reason, he uses a pretrained model. But he provides the (commented) code for the training, which is displayed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pretrained model (basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one has trained a first time the model, one can reuse it during the next executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model on test data (basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure one improves the quality of the model without overfitting, one has to test it agains test data. In the tutorial, they advise to alternate training phases on training data and testing phases on test data, and stop when the accuracy on test data starts decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the accuracy of the model against the whole test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_test = int(numFilesTest/batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTest = len(positiveFilesTest)\n",
    "lenNegTest = len(negativeFilesTest)\n",
    "labelsTest = [[1,0] for i in range(lenPosTest)] + [[0,1] for i in range(lenNegTest)]\n",
    "accuracy_test = np.zeros(n_iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestBatchOrder(i):\n",
    "    nextBatchLabels = labelsTest[(i):(i+batchSize)]\n",
    "    nextBatch = np.zeros([batchSize, maxSeqLength])\n",
    "    for j in range(batchSize):\n",
    "        nextBatch[j] = ids_test[(i+j):(i+j+1)]\n",
    "    return nextBatch, nextBatchLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iter_test):\n",
    "    nextBatch, nextBatchLabels = getTestBatchOrder(i)\n",
    "    accuracy_test[i] = (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set = \", accuracy_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells I try to access directly what is happening when I call the train method of my Estimator object. This allows me to understand source of errors and warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a graph which corresponds to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Gather:0\", shape=(?, 64), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(64, 2) dtype=float32_ref>\n",
      "Tensor(\"add:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"sparse_softmax_cross_entropy_loss/value:0\", shape=(), dtype=float32)\n",
      "(<tf.Tensor 'acc_op/value:0' shape=() dtype=float32>, <tf.Tensor 'acc_op/update_op:0' shape=() dtype=float32>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "dataset = train_input_fn(features=ids_train_toy, labels=y_train_toy_ord, batch_size=batch_size)\n",
    "features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "input_data = tf.feature_column.input_layer(features, my_feature_columns)\n",
    "input_data = tf.cast(input_data, tf.int32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "data = tf.cast(data, tf.float32)\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(64)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.8)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "print(last)\n",
    "weight = tf.Variable(tf.truncated_normal([64, 2]))\n",
    "print(weight)\n",
    "bias = tf.Variable(tf.constant(0.1, shape=(2,)))\n",
    "logits = (tf.matmul(last, weight) + bias)\n",
    "print(logits)\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "print(loss)\n",
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "print(accuracy)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('accuracy', accuracy[1])\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Merge_1/MergeSummary:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_debug = tf.InteractiveSession()\n",
    "sess_debug.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason it seems to be necessary to add the following line though it wasn't in the original script. Otherwise I optain a FailedPreconditionError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_debug.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_debug = tf.summary.FileWriter(model_dir, sess_debug.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #Next Batch of reviews\n",
    "    sess_debug.run(train_op, {})\n",
    "\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 10 == 0):\n",
    "        print(i)\n",
    "        summary = sess_debug.run(merged, {})\n",
    "        writer_debug.add_summary(summary, i)\n",
    "writer_debug.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
