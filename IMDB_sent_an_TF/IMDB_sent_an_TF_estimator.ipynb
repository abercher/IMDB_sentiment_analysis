{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB reviews: TensorFlow GloVe and LSTM with Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to perform sentiment analysis using TensorFlow. Most of the notebook is a variation of what was done on this blog:\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "This is an upgrade of the previous notebook (IMDB_sent_an_TF_basic_improved1) where I'm replacing the basic APIs by custom Estimator level APIs. For this I follow the indications of these tutorial:\n",
    "\n",
    "https://www.tensorflow.org/get_started/premade_estimators\n",
    "\n",
    "https://www.tensorflow.org/get_started/datasets_quickstart\n",
    "\n",
    "https://www.tensorflow.org/get_started/custom_estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "In this notebook, I use the smallest ones, i.e. the ones where the word embedding vectors are of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_file_name = 'glove.6B/glove.6B.50d_toy.txt' #toy embeddings with only the\n",
    "                                                #three first rows (instead of 4K)\n",
    "emb_file_name = 'glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the next cell comes from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "emb_df = pd.read_table(emb_file_name, sep=\" \",\n",
    "                       index_col=0, header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the Pandas data frame into one list containing the words (the indexes from the data frame) and one numpy array containing the corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = list(emb_df.index)\n",
    "wordVectors = emb_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n",
       "       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n",
       "       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n",
       "       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n",
       "       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n",
       "        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n",
       "        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n",
       "       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n",
       "       -0.70315 ,  0.17157 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[   41   804     0  1005    15  7446     5 13767     0     0]\n"
     ]
    }
   ],
   "source": [
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are looking for the threshold we should take as maximum length of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_POS = TRAIN + 'pos/'\n",
    "TRAIN_NEG = TRAIN + 'neg/'\n",
    "positiveFilesTrain = [TRAIN_POS + f for f in listdir(TRAIN_POS) if isfile(join(TRAIN_POS, f))]\n",
    "negativeFilesTrain = [TRAIN_NEG + f for f in listdir(TRAIN_NEG) if isfile(join(TRAIN_NEG, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_POS = TEST + 'pos/'\n",
    "TEST_NEG = TEST + 'neg/'\n",
    "positiveFilesTest = [TEST_POS + f for f in listdir(TEST_POS) if isfile(join(TEST_POS, f))]\n",
    "negativeFilesTest = [TEST_NEG + f for f in listdir(TEST_NEG) if isfile(join(TEST_NEG, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine the average number of words in one sample of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "numWords = []\n",
    "for pf in positiveFilesTrain:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFilesTrain:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFilesTrain = len(positiveFilesTrain) + len(negativeFilesTrain)\n",
    "numFilesTest = len(positiveFilesTest) + len(negativeFilesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely it seems that there aren't exactly 12500 files in the folders indicated below, as it is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n",
      "12501\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveFilesTrain))\n",
    "print(len(negativeFilesTrain))\n",
    "print(len(positiveFilesTest))\n",
    "print(len(negativeFilesTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 25002\n",
      "The total number of words in the files is 5844682\n",
      "The average number of words in the files is 233.7685785137189\n"
     ]
    }
   ],
   "source": [
    "print('The total number of files is', numFilesTrain)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an histogram of the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, 250 seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we map words to indexes using our wordsList variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do it for a specific file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An excellent movie about two cops loving the same woman. One of the cop (Périer) killed her, but all the evidences seems to incriminate the other (Montand). The unlucky Montand doesnt know who is the other lover that could have killed her, and Périer doesnt know either that Montand had an affair with the girl. Montand must absolutely find the killer...and what a great ending! Highly recommended.\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFilesTrain[3] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    29,   4345,   1005,     59,     55,  12193,   8842,      0,\n",
       "          215,    787,     48,      3,      0,  10417, 388445,    256,\n",
       "           71,     34,     64,      0,  47019,   1348,      4,  59799,\n",
       "            0,     68, 105903,      0,  20938, 105903, 136283,    346,\n",
       "           38,     14,      0,     68,   8410,     12,     94,     33,\n",
       "          256,     71,      5, 388445, 136283,    346,    900,     12,\n",
       "       105903,     40,     29,   4160,     17,      0,   1749, 105903,\n",
       "          390,   3960,    596,      0, 399999,    102,      7,    353,\n",
       "         1945,   1786,   3885,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids_train will be a matrix, which contains for each file of the training set (first the positive ones, then the negative ones) a row where the columns contain the indices corresponding to the words of the sample file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_train = np.zeros((numFilesTrain, maxSeqLength), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCounter = 0\n",
    "#for pf in positiveFilesTrain:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFilesTrain:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "#np.save('idsMatrixTrain', ids_train)\n",
    "\n",
    "ids_train = np.load('idsMatrixTrain.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(25002, 250)\n",
      "[ 12588     14     29 399999    319     34     20     14   2242  31631\n",
      "   2094  11013  17614     22     47   1058     14     20     14      7\n",
      " 399999    319     42 255441     30    573    100   1299    296      0\n",
      " 399999  30748      3   2838   4450  13819      0    402   4184     17\n",
      "      0   4442  14959  10220      5  12588  60607  12256      0 399999\n",
      "     34   2909  47119    461   4543   1749     25      0  12626   3752\n",
      "      5   6801      4      0   2037   9742   1174     14  34443      5\n",
      " 399999      0   3226   4260   1654    107    339     77    138     22\n",
      "     58     34     20     14 130086   5610      5  16089      0  23277\n",
      "   8652     42     14   7206    983     37    319     42   5635    109\n",
      "    615     34   1952    306  97532   5319   2909  14014      6   2158\n",
      "   5418      3      5  12534     10  12588      5      0    109     38\n",
      "    593     71   9919  42692      4    155     38   1764      4    169\n",
      "     20    116      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "print(type(ids_train))\n",
    "print(ids_train.shape)\n",
    "print(ids_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some toy data to be able to perform quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(200, 250)\n",
      "[  3936     48      3    192   8489   2459      0    523     14    219\n",
      "   2050     14    353     64  63305    858  22181     32   2664      5\n",
      "      0   5230     14  11114  10101  15951     14  11025     19   2701\n",
      "  13502   3755     18     14  20355     19      0   6302  13898      5\n",
      "     14   1807    143     21  12070   3992  17748  10958      5   5272\n",
      "  11690     64    608  11184     32     13    389      0    319     14\n",
      " 399999   1633   3496    248     60      4      4      0    156     48\n",
      "      3    192   8489   1588     25      0    288   4754  10101  15951\n",
      "     38     31     51      0   7255      3    359    319     18     31\n",
      "   5938      6      5    907     84   3895    187     29   4345    319\n",
      "     64    469      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "ids_train_toy = ids_train[12400:12600]\n",
    "print(type(ids_train_toy))\n",
    "print(ids_train_toy.shape)\n",
    "print(ids_train_toy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_test = np.zeros((numFilesTest, maxSeqLength), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCounter = 0\n",
    "#for pf in positiveFilesTest:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFilesTest:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('idsMatrixTest', ids_test)\n",
    "\n",
    "ids_test = np.load('idsMatrixTest.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also create the **labels** with **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative), as done in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTrain = len(positiveFilesTrain)\n",
    "lenNegTrain = len(negativeFilesTrain)\n",
    "y_train = [[1,0] for i in range(lenPosTrain)] + [[0,1] for i in range(lenNegTrain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTest = len(positiveFilesTest)\n",
    "lenNegTest = len(negativeFilesTest)\n",
    "y_test = [[1,0] for i in range(lenPosTest)] + [[0,1] for i in range(lenNegTest)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the labels for the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_toy = y_train[12400:12600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some premade estimators do not accept one-hot-encoding of the labels as explained here:https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "\n",
    "For this reason I create a function which converts a list containing one-hot-incoded labels into a list containing **ordinal encoded** labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot2Ordinal(oneHot):\n",
    "    n_classes = len(oneHot[0])\n",
    "    ordinal = []\n",
    "    for i in range(len(oneHot)):\n",
    "        for j in range(n_classes):\n",
    "            if oneHot[i][j]==1:\n",
    "                ordinal.append(j)\n",
    "    return(ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_train_toy_ord = oneHot2Ordinal(y_train_toy)\n",
    "print(len(y_train_toy_ord))\n",
    "print(y_train_toy_ord[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ord = oneHot2Ordinal(y_train)\n",
    "y_test_ord = oneHot2Ordinal(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating batching functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create two functions which will help feeding the model with batches of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original implementation of getTrainBatch would only use part of the training data for the training phase (indices 0 to 11498 for positive, instead of 0 to 12498, and indices 13499 to 24999 for negatives instead of 12499 to 24999). Since it is not clear exactly where the indices of the positive reviews stop and when the indices of the negative reviews start, I take stop at 12499 for positive and starts at 12502 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,12499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(12502,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids_train[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is actually pretty useless. I will replace it by a function which tests the model against the whole test data (and not just against some part of the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids_train[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllTest():\n",
    "    lenPosTest = len(positiveFilesTest)\n",
    "    lenNegTest = len(negativeFilesTest)\n",
    "    labels = [[0,1] for i in range(lenPosTest)] + [[1,0] for i in range(lenNegTest)]\n",
    "    return ids_test, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a model using the Estimator APIs from TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the **input functions**. They are the objects which supply data for training, evaluating, and prediction to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'Indexes':features}, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(1000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the train_input_fn works well, this **should be modified in the same way** by using a dictionary in from_tensor_slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = {'Indexes':features}\n",
    "    else:\n",
    "        inputs = ({'Indexes':features}, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **feature columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = []\n",
    "my_feature_columns.append(tf.feature_column.numeric_column(key='Indexes', shape=maxSeqLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a **premade estimator**. This will be changed later into a custom estimator once I will have tested it and made sure that my input function and feature columns are correctly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpg8rlfpgj\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_log_step_count_steps': 100, '_task_id': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpg8rlfpgj', '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efbd6511d68>, '_master': '', '_num_worker_replicas': 1, '_evaluation_master': '', '_save_summary_steps': 100, '_num_ps_replicas': 0, '_save_checkpoints_steps': None, '_global_id_in_cluster': 0, '_task_type': 'worker', '_keep_checkpoint_max': 5, '_is_chief': True, '_session_config': None}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[3, 3],\n",
    "    # The model must choose between 2 classes.\n",
    "    n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(100)\n",
    "train_steps = int(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the premade estimator that we are using requires the label to be ordinaly encoded and not one-hot-encoded as explained here:\n",
    "\n",
    "https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpg8rlfpgj/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 70522.86\n",
      "INFO:tensorflow:global_step/sec: 528.741\n",
      "INFO:tensorflow:step = 101, loss = 65.876564 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 874.883\n",
      "INFO:tensorflow:step = 201, loss = 66.16997 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 994.37\n",
      "INFO:tensorflow:step = 301, loss = 66.38655 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 1019.05\n",
      "INFO:tensorflow:step = 401, loss = 66.679474 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1147.13\n",
      "INFO:tensorflow:step = 501, loss = 67.04363 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1140.81\n",
      "INFO:tensorflow:step = 601, loss = 66.09365 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1099.45\n",
      "INFO:tensorflow:step = 701, loss = 65.8028 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.489\n",
      "INFO:tensorflow:step = 801, loss = 66.753944 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 1020.45\n",
      "INFO:tensorflow:step = 901, loss = 66.53334 (0.098 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpg8rlfpgj/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 65.80284.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7efbd6511cf8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(features=ids_train_toy, labels=y_train_toy_ord, batch_size=100),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we present the original implementation of the model, relying on TF basic APIs. It will be replaced by estimators APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These placeholders are here to take the input of the model (labels and samples turned into arrays of indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we embed the indices into vectors. The next cell is commented out because I think that it is useless. It was in the tutorial but I suspect that its author forgot to remove it.\n",
    "As explained in the tutorial, we were using pretrained embeddings where vectors have length 50. But here numDimensions are of length 300. And in the following cell, 'data' defined again... I ran the notebook with and without it and it gives similar results so I commented it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a fix comming from\n",
    "https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/issues/2\n",
    "If I don't put it, errors appear in the cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.cast(data, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the LSTM with dropout layer. According to the tutorial, the parameter lstmUnits needs some tuning to find the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aritz/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I understood it right, 'value' in the next cell represents the outputs of the lstm (for each sample of the batch and each word of each sample). According to the documentation it should have dimensions equal to [batch_size, max_time, cell.output_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add some afine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm not mistaken the next cell swaps the two first dimensions so it has dimensions [max_time, batch_size, cell.output_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf.transpose(value, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm not mistaken the next cell slices the part of the output which corresponds to the last output of the lstm, or in other words the output corresponding to the last word for every sample (if I'm right we used 0 padding and cut everything which goes beyound 250 words, so technically it is the 250th output). My guess is that last has dimensions [batch_size, cell.output_size] which we can then use to do matrix multiplication with weight which has dimensions [cell.output_size, numClasses] (remember that cell.output_size=lstmUnits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the cross-entropy loss using the logits (i.e. unnormalized probabilities), and we define the optimizer. Note that I replaced tf.nn.softmax_cross_entropy_with_logits (as in the original script) by tf.nn.softmax_cross_entropy_with_logits_v2 as indicated by a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows us to use TensorBoard to visualize the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterations gives the number of batches against whom we are going to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the author of the tutorial I'm copying, the training takes a lot of time. For this reason, he uses a pretrained model. But he provides the (commented) code for the training, which is displayed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#saver = tf.train.Saver()\n",
    "#sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(iterations):\n",
    "#    #Next Batch of reviews\n",
    "#    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "#    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#\n",
    "#    #Write summary to Tensorboard\n",
    "#    if (i % 50 == 0):\n",
    "#        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#        writer.add_summary(summary, i)\n",
    "#\n",
    "#    #Save the network every 10,000 training iterations\n",
    "#    if (i % 10000 == 0 and i != 0):\n",
    "#        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#        print(\"saved to %s\" % save_path)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author of the tutorial also mentions the possibility of tracking the progress of the model on TensorBoard by entering \"tensorboard --logdir=tensorboard\" in a terminal, and visiting http://localhost:6006/ with a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one has trained a first time the model, one can reuse it during the next executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure one improves the quality of the model without overfitting, one has to test it agains test data. In the tutorial, they advise to alternate training phases on training data and testing phases on test data, and stop when the accuracy on test data starts decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the accuracy of the model against the whole test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_test = int(numFilesTest/batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTest = len(positiveFilesTest)\n",
    "lenNegTest = len(negativeFilesTest)\n",
    "labelsTest = [[1,0] for i in range(lenPosTest)] + [[0,1] for i in range(lenNegTest)]\n",
    "accuracy_test = np.zeros(n_iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestBatchOrder(i):\n",
    "    nextBatchLabels = labelsTest[(i):(i+batchSize)]\n",
    "    nextBatch = np.zeros([batchSize, maxSeqLength])\n",
    "    for j in range(batchSize):\n",
    "        nextBatch[j] = ids_test[(i+j):(i+j+1)]\n",
    "    return nextBatch, nextBatchLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iter_test):\n",
    "    nextBatch, nextBatchLabels = getTestBatchOrder(i)\n",
    "    accuracy_test[i] = (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set =  85.53874476613275\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test set = \", accuracy_test.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
