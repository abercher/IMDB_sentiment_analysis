{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB reviews: TensorFlow GloVe and LSTM (basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to perform sentiment analysis using TensorFlow. Most of the notebook is a copy of what was done on this blog:\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "Everything is implemented \"manualy\" and it will be a good basis to go toward something more refined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "In this notebook, I use the smallest ones, i.e. the ones where the embedding vectors are of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_file_name = 'glove.6B/glove.6B.50d_toy.txt' #toy embeddings with only the\n",
    "                                                #three first rows (instead of 4K)\n",
    "emb_file_name = 'glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the next cell comes from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "emb_df = pd.read_table(emb_file_name, sep=\" \",\n",
    "                       index_col=0, header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the Pandas data frame into one list containing the words (the indexes from the data frame) and one numpy array containing the corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = list(emb_df.index)\n",
    "wordVectors = emb_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n",
       "       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n",
       "       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n",
       "       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n",
       "       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n",
       "        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n",
       "        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n",
       "       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n",
       "       -0.70315 ,  0.17157 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line should be removed. I let it here because it was in the original tutorial but it serves no purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numDimensions = 300 #Dimensions for each word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[   41   804     0  1005    15  7446     5 13767     0     0]\n"
     ]
    }
   ],
   "source": [
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are looking for the threshold we should take as maximum length of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_POS = TRAIN + 'pos/'\n",
    "TRAIN_NEG = TRAIN + 'neg/'\n",
    "positiveFiles = [TRAIN_POS + f for f in listdir(TRAIN_POS) if isfile(join(TRAIN_POS, f))]\n",
    "negativeFiles = [TRAIN_NEG + f for f in listdir(TRAIN_NEG) if isfile(join(TRAIN_NEG, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine the average number of words in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 25002\n",
      "The total number of words in the files is 5844682\n",
      "The average number of words in the files is 233.7685785137189\n"
     ]
    }
   ],
   "source": [
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an histogram of the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, 250 seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we map words to indexes using our wordsList variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do it for a specific file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An excellent movie about two cops loving the same woman. One of the cop (Périer) killed her, but all the evidences seems to incriminate the other (Montand). The unlucky Montand doesnt know who is the other lover that could have killed her, and Périer doesnt know either that Montand had an affair with the girl. Montand must absolutely find the killer...and what a great ending! Highly recommended.\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    29,   4345,   1005,     59,     55,  12193,   8842,      0,\n",
       "          215,    787,     48,      3,      0,  10417, 388445,    256,\n",
       "           71,     34,     64,      0,  47019,   1348,      4,  59799,\n",
       "            0,     68, 105903,      0,  20938, 105903, 136283,    346,\n",
       "           38,     14,      0,     68,   8410,     12,     94,     33,\n",
       "          256,     71,      5, 388445, 136283,    346,    900,     12,\n",
       "       105903,     40,     29,   4160,     17,      0,   1749, 105903,\n",
       "          390,   3960,    596,      0, 399999,    102,      7,    353,\n",
       "         1945,   1786,   3885,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for all the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "#fileCounter = 0\n",
    "#for pf in positiveFiles:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFiles:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "#np.save('idsMatrix', ids)\n",
    "\n",
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating batching functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create two functions which will help feeding the model with batches of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personnaly I don't like very much this implementation as it implies that we cannot define clear periods over which we go through all the data one and only one time. This is a point I would like to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These placeholders are here to take the input of the model (labels and samples turned into arrays of indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we embed the indices into vectors. The next cell is commented out because I think that it is useless. It was in the tutorial but I suspect that its author forgot to remove it.\n",
    "As explained in the tutorial, we were using pretrained embeddings where vectors have length 50. But here numDimensions are of length 300. And in the following cell, 'data' defined again... I ran the notebook with and without it and it gives similar results so I commented it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a fix comming from\n",
    "https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/issues/2\n",
    "If I don't put it, errors appear in the cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.cast(data, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the LSTM with dropout layer. According to the tutorial, the parameter lstmUnits needs some tuning to find the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aritz/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I understood it right, 'value' in the next cell represents the outputs of the lstm (for each sample of the batch and each word of each sample). According to the documentation it should have dimensions equal to [batch_size, max_time, cell.output_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add some afine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm not mistaken the next cell swaps the two first dimensions so it has dimensions [max_time, batch_size, cell.output_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf.transpose(value, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm not mistaken the next cell slices the part of the output which corresponds to the last output of the lstm, or in other words the output corresponding to the last word for every sample (if I'm right we used 0 padding and cut everything which goes beyound 250 words, so technically it is the 250th output). My guess is that last has dimensions [batch_size, cell.output_size] which we can then use to do matrix multiplication with weight which has dimensions [cell.output_size, numClasses] (remember that cell.output_size=lstmUnits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the cross-entropy loss using the logits (i.e. unnormalized probabilities), and we define the optimizer. Note that I replaced tf.nn.softmax_cross_entropy_with_logits (as in the original script) by tf.nn.softmax_cross_entropy_with_logits_v2 as indicated by a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows us to use TensorBoard to visualize the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of iterations in the tutorial was 100K but it seems a lot. I will start with smaller values (1k, 10K) and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the author of the tutorial I'm copying, the training takes a lot of time. For this reason, he uses a pretrained model. But he provides the (commented) code for the training, which is displayed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author of the tutorial also mentions the possibility of tracking the progress of the model on TensorBoard by entering \"tensorboard --logdir=tensorboard\" in a terminal, and visiting http://localhost:6006/ with a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one has trained a first time the model, one can reuse it during the next executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure one improves the quality of the model without overfitting, one has to test it agains test data. In the tutorial, they advise to alternate training phases on training data and testing phases on test data, and stop when the accuracy on test data starts decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 58.33333134651184\n"
     ]
    }
   ],
   "source": [
    "iterations = 30\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
