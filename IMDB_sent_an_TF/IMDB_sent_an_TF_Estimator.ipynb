{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB reviews: TensorFlow GloVe and LSTM with Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to perform sentiment analysis using TensorFlow. Most of the notebook is a variation of what was done on this blog:\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "This is an upgrade of the previous notebook (IMDB_sent_an_TF_basic_improved1) where I'm replacing the basic APIs by custom Estimator level APIs. For this I follow the indications of these tutorials coming from the official documentation:\n",
    "\n",
    "https://www.tensorflow.org/get_started/premade_estimators\n",
    "\n",
    "https://www.tensorflow.org/get_started/datasets_quickstart\n",
    "\n",
    "https://www.tensorflow.org/get_started/custom_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import tensorflow as tf\n",
    "#import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "In this notebook, I use the smallest ones, i.e. the ones where the word embedding vectors are of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_file_name = 'glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the next cell comes from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "emb_df = pd.read_table(emb_file_name, sep=\" \",\n",
    "                       index_col=0, header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the Pandas data frame into one list `wordsList` containing the words (the indexes from the data frame) and one numpy array `wordVectors` containing the corresponding vectors. This last data frame will play the role of our **look-up table** later when we define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = list(emb_df.index)\n",
    "wordVectors = emb_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I commented this part because it is useless for the real task. It only serves a pedagogic purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseballIndex = wordsList.index('baseball')\n",
    "#wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxSeqLength = 10 #Maximum length of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "#firstSentence[0] = wordsList.index(\"i\")\n",
    "#firstSentence[1] = wordsList.index(\"thought\")\n",
    "#firstSentence[2] = wordsList.index(\"the\")\n",
    "#firstSentence[3] = wordsList.index(\"movie\")\n",
    "#firstSentence[4] = wordsList.index(\"was\")\n",
    "#firstSentence[5] = wordsList.index(\"incredible\")\n",
    "#firstSentence[6] = wordsList.index(\"and\")\n",
    "#firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "##firstSentence[8] and firstSentence[9] are going to be 0\n",
    "#print(firstSentence.shape)\n",
    "#print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are looking for the threshold we should take as maximum length of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_POS = TRAIN + 'pos/'\n",
    "TRAIN_NEG = TRAIN + 'neg/'\n",
    "positiveFilesTrain = [TRAIN_POS + f for f in listdir(TRAIN_POS) if isfile(join(TRAIN_POS, f))]\n",
    "negativeFilesTrain = [TRAIN_NEG + f for f in listdir(TRAIN_NEG) if isfile(join(TRAIN_NEG, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_POS = TEST + 'pos/'\n",
    "TEST_NEG = TEST + 'neg/'\n",
    "positiveFilesTest = [TEST_POS + f for f in listdir(TEST_POS) if isfile(join(TEST_POS, f))]\n",
    "negativeFilesTest = [TEST_NEG + f for f in listdir(TEST_NEG) if isfile(join(TEST_NEG, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine the average number of words in one sample of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "numWords = []\n",
    "for pf in positiveFilesTrain:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFilesTrain:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFilesTrain = len(positiveFilesTrain) + len(negativeFilesTrain)\n",
    "numFilesTest = len(positiveFilesTest) + len(negativeFilesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely it seems that there aren't exactly 12500 files in the folders indicated below, as it is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n",
      "12501\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveFilesTrain))\n",
    "print(len(negativeFilesTrain))\n",
    "print(len(positiveFilesTest))\n",
    "print(len(negativeFilesTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 25002\n",
      "The total number of words in the files is 5844682\n",
      "The average number of words in the files is 233.7685785137189\n"
     ]
    }
   ],
   "source": [
    "print('The total number of files is', numFilesTrain)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an histogram of the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, 250 words seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we map words to indexes using our wordsList variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do it for a specific file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the content of the file (i.e. the real text).  (I commented out this part because it is useless for the real task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = positiveFilesTrain[3] #Can use any valid index (not just 3)\n",
    "#with open(fname) as f:\n",
    "#    for lines in f:\n",
    "#        print(lines)\n",
    "#        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the given file to a list of indexes where each indexe corresponds to a word, according to the list `wordsList`. (I commented out this part because it is useless for the real task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "#with open(fname) as f:\n",
    "#    indexCounter = 0\n",
    "#    line=f.readline()\n",
    "#    cleanedLine = cleanSentences(line)\n",
    "#    split = cleanedLine.split()\n",
    "#    for word in split:\n",
    "#        try:\n",
    "#            firstFile[indexCounter] = wordsList.index(word)\n",
    "#        except ValueError:\n",
    "#            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "#        indexCounter = indexCounter + 1\n",
    "#firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids_train will be a matrix, which contains for each file of the training set (first the positive ones, then the negative ones) a row where the columns contain the indices corresponding to the words of the sample file.\n",
    "\n",
    "The whole computation of the transformation of each text file into a list of indices takes time and needs to be performed only once. The result is saved after the first time and then reloaded for all the other executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_train = np.zeros((numFilesTrain, maxSeqLength), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCounter = 0\n",
    "#for pf in positiveFilesTrain:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFilesTrain:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "#np.save('idsMatrixTrain', ids_train)\n",
    "\n",
    "ids_train = np.load('idsMatrixTrain.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some toy data to be able to perform quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train_toy = ids_train[12400:12600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_test = np.zeros((numFilesTest, maxSeqLength), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCounter = 0\n",
    "#for pf in positiveFilesTest:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in negativeFilesTest:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('idsMatrixTest', ids_test)\n",
    "\n",
    "ids_test = np.load('idsMatrixTest.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also create the **labels** with **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative), as done in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTrain = len(positiveFilesTrain)\n",
    "lenNegTrain = len(negativeFilesTrain)\n",
    "y_train = [[1,0] for i in range(lenPosTrain)] + [[0,1] for i in range(lenNegTrain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenPosTest = len(positiveFilesTest)\n",
    "lenNegTest = len(negativeFilesTest)\n",
    "y_test = [[1,0] for i in range(lenPosTest)] + [[0,1] for i in range(lenNegTest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenTrain = lenPosTrain + lenNegTrain\n",
    "lenTest = lenPosTest + lenNegTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the labels for the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_toy = y_train[12400:12600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some premade estimators do not accept one-hot-encoding of the labels as explained here:https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "\n",
    "For this reason I create a function which converts a list containing one-hot-incoded labels into a list containing **ordinal encoded** labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot2Ordinal(oneHot):\n",
    "    n_classes = len(oneHot[0])\n",
    "    ordinal = []\n",
    "    for i in range(len(oneHot)):\n",
    "        for j in range(n_classes):\n",
    "            if oneHot[i][j]==1:\n",
    "                ordinal.append(j)\n",
    "    return(ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y_train_toy_ord = oneHot2Ordinal(y_train_toy)\n",
    "print(len(y_train_toy_ord))\n",
    "print(y_train_toy_ord[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ord = oneHot2Ordinal(y_train)\n",
    "y_test_ord = oneHot2Ordinal(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a model using the Estimator APIs from TF. One of the advantage of this higher level of APIs is that some things done manualy when using TF basic APIs, are done automatically. There is no need to initialize variables for instance, or defining writers for TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the **input functions**. They are the objects which supply data for training, evaluating, and prediction to the model. They are using `tf.data.Dataset` objects which are one of the key tools of TF. These objects allow to access the data and manipulate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the body of the next function, it is important that the argument of the `shuffle` method is equal to the length of the whole training data set. See entry of the 11.07.18 of my journal for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'Indexes':features}, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(lenTrain).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = {'Indexes':features}\n",
    "    else:\n",
    "        inputs = ({'Indexes':features}, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **feature columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = []\n",
    "my_feature_columns.append(tf.feature_column.numeric_column(key='Indexes', shape=maxSeqLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the directory where to store the log files for TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define a custom estimator we need to define a **model function**. For this we mix the code of the notebook based only on basic TF APIs together with some parts of the script of this tutorial: https://www.tensorflow.org/get_started/custom_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    # Use `input_layer` to apply the feature columns.\n",
    "    input_data = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # The next line is required because tf.feature_column.input_layer\n",
    "    # outputs tf.float32 (whatever the input)\n",
    "    # and tf.nn.embedding_lookup requires\n",
    "    # tf.int32\n",
    "    input_data = tf.cast(input_data, tf.int32)\n",
    "    # Transform each index in a sentence into the associated vector\n",
    "    data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "    # The following line is a fixe coming from this page:\n",
    "    # https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/issues/2\n",
    "    # in order to prevent an error appearing next.\n",
    "    data = tf.cast(data, tf.float32)\n",
    "    # Next we define the LSTM\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(params['lstmUnits'])\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=params['keep_prob'])\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "    # swaps the two first dimensions so it has dimensions [max_time, batch_size, cell.output_size]\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    # If I'm not mistaken the next cell slices the part of \n",
    "    # the output which corresponds to the last output of the lstm, \n",
    "    # or in other words the output corresponding to the \n",
    "    # last word for every sample (if I'm right we used \n",
    "    # 0 padding and cut everything which goes beyound 250 words, \n",
    "    # so technically it is the 250th output). \n",
    "    # My guess is that last has dimensions [batch_size, cell.output_size] \n",
    "    # which we can then use to do matrix multiplication \n",
    "    # with weight which has dimensions [cell.output_size, numClasses] \n",
    "    # (remember that cell.output_size=lstmUnits).\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    # We apply an affine transformation to get the logits\n",
    "    weight = tf.Variable(tf.truncated_normal([params['lstmUnits'], params['n_classes']]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[params['n_classes']]))\n",
    "    logits = (tf.matmul(last, weight) + bias)\n",
    "    # Maybe I could  replace this last part using tf.layers.dense:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "    \n",
    "    # The following lines are actually independent of the achitecture\n",
    "    # of the model.\n",
    "    \n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Compute loss\n",
    "    \n",
    "    # Note that because of this function, we have to\n",
    "    # provide ordinaly encoded labels and not one-hot-encoded\n",
    "    # labels, as explained on this page:\n",
    "    # https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define the **custom estimator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_task_type': 'worker', '_save_checkpoints_steps': None, '_global_id_in_cluster': 0, '_is_chief': True, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_log_step_count_steps': 100, '_tf_random_seed': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_model_dir': '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc93084c780>, '_service': None, '_num_ps_replicas': 0, '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        model_dir=model_dir,\n",
    "        params={\n",
    "            'feature_columns': my_feature_columns,\n",
    "            'n_classes': 2,\n",
    "            'lstmUnits': 64,\n",
    "            'keep_prob': 0.8\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation of the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(100)\n",
    "train_steps = int(4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can **train** our Estimator. Note that for the function `tf.losses.sparse_softmax_cross_entropy`, that we are using in the model function, requires the **label** to be **ordinaly encoded and not one-hot-encoded** as explained here:\n",
    "\n",
    "https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/aritz/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2752001, step = 0\n",
      "INFO:tensorflow:global_step/sec: 5.9635\n",
      "INFO:tensorflow:loss = 0.7041593, step = 100 (16.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.96191\n",
      "INFO:tensorflow:loss = 0.7104996, step = 200 (14.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.93461\n",
      "INFO:tensorflow:loss = 0.6867763, step = 300 (14.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.38648\n",
      "INFO:tensorflow:loss = 0.72279143, step = 400 (15.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.59159\n",
      "INFO:tensorflow:loss = 0.686315, step = 500 (17.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.62637\n",
      "INFO:tensorflow:loss = 0.70281607, step = 600 (15.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.32671\n",
      "INFO:tensorflow:loss = 0.6822139, step = 700 (15.804 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.68738\n",
      "INFO:tensorflow:loss = 0.69449943, step = 800 (14.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.90345\n",
      "INFO:tensorflow:loss = 0.6387156, step = 900 (14.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.97221\n",
      "INFO:tensorflow:loss = 0.5976469, step = 1000 (14.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.51942\n",
      "INFO:tensorflow:loss = 0.48926777, step = 1100 (15.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.69206\n",
      "INFO:tensorflow:loss = 0.4951453, step = 1200 (14.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.9826\n",
      "INFO:tensorflow:loss = 0.3965807, step = 1300 (14.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.7574\n",
      "INFO:tensorflow:loss = 0.5713916, step = 1400 (14.799 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.9564\n",
      "INFO:tensorflow:loss = 0.44204295, step = 1500 (14.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46398\n",
      "INFO:tensorflow:loss = 0.39956963, step = 1600 (15.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.00477\n",
      "INFO:tensorflow:loss = 0.44843745, step = 1700 (14.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3217\n",
      "INFO:tensorflow:loss = 0.46249148, step = 1800 (15.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.00955\n",
      "INFO:tensorflow:loss = 0.4081302, step = 1900 (14.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.94717\n",
      "INFO:tensorflow:loss = 0.44927177, step = 2000 (14.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.97495\n",
      "INFO:tensorflow:loss = 0.35253665, step = 2100 (14.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.91543\n",
      "INFO:tensorflow:loss = 0.40019166, step = 2200 (14.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.02702\n",
      "INFO:tensorflow:loss = 0.38226074, step = 2300 (14.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.07449\n",
      "INFO:tensorflow:loss = 0.38541633, step = 2400 (14.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.06226\n",
      "INFO:tensorflow:loss = 0.36306286, step = 2500 (14.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.79209\n",
      "INFO:tensorflow:loss = 0.37870818, step = 2600 (14.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3226\n",
      "INFO:tensorflow:loss = 0.3279792, step = 2700 (15.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.5749\n",
      "INFO:tensorflow:loss = 0.41140214, step = 2800 (15.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.72671\n",
      "INFO:tensorflow:loss = 0.41723105, step = 2900 (14.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.95656\n",
      "INFO:tensorflow:loss = 0.3737265, step = 3000 (14.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.9409\n",
      "INFO:tensorflow:loss = 0.4407811, step = 3100 (14.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.87395\n",
      "INFO:tensorflow:loss = 0.36162147, step = 3200 (14.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.38633\n",
      "INFO:tensorflow:loss = 0.33628762, step = 3300 (15.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.3156\n",
      "INFO:tensorflow:loss = 0.3187706, step = 3400 (15.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17631\n",
      "INFO:tensorflow:loss = 0.31626338, step = 3500 (16.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.679\n",
      "INFO:tensorflow:loss = 0.39689407, step = 3600 (14.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.47644\n",
      "INFO:tensorflow:loss = 0.26145852, step = 3700 (15.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27055\n",
      "INFO:tensorflow:loss = 0.4052645, step = 3800 (15.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 7.22753\n",
      "INFO:tensorflow:loss = 0.36234558, step = 3900 (13.836 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.41363728.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fc93084cc88>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(features=ids_train, labels=y_train_ord, batch_size=batch_size),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-12-13:22:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-12-13:22:37\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.8103676, global_step = 4000, loss = 0.41759548\n"
     ]
    }
   ],
   "source": [
    "eval_test = classifier.evaluate(input_fn=lambda:eval_input_fn(features=ids_test,\n",
    "                                                                    labels=y_test_ord,\n",
    "                                                                    batch_size=batch_size)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we print the **accuracy of the model on the test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track the progress of the model on TensorBoard it is enough to enter \"tensorboard --logdir (...)\" in a terminal with \"(...)\" replaced by the name of the directory where the event files are saved, and visiting http://localhost:6006/ with a browser.\n",
    "\n",
    "With custom estimators it is enough to include lines like\n",
    "`tf.summary.scalar('loss', loss)`\n",
    "in the model function in order to track the quantities we are interested in. An event file for TensorBoard will be updated every 100 steps during the training with the `train` method (at least it is what I observe from my empirical experience), and a single event file measuring the state of the tracked variable will be written when calling the `evaluate`method. There is no need to define a writer with `tf.summary.FileWriter`.\n",
    "\n",
    "**Caveat emptor:** This being said, I often ran into problems with TensorBoard. No files were being written, or only during the evaluation phase. Currently it seems to work as I described above, but I cannot garantee that what I described is absolutely true. I don't know yet how to specify the behaviour of an Estimator object when it comes to TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells I use TF basic APIs to access directly what is happening when I call the train method of my Estimator object. This allows me to understand source of errors and warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(10)\n",
    "lstmUnits = int(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining **two datasets**. One for the training and one for the testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_input_fn(features=ids_train, labels=y_train_ord, batch_size=batch_size)\n",
    "dataset_test = eval_input_fn(features=ids_test, labels=y_test_ord, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **reinitializable iterator**. Unlike One-shot iterators, they alow to switch from one dataset to another one. As explained here:\n",
    "https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator, \n",
    "\"A reinitializable iterator is defined by its structure. We could use the\n",
    " `output_types` and `output_shapes` properties of either `dataset_train`\n",
    " or `dataset_test` here, because they are compatible.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                           dataset_train.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the model itself (this part is similar to what is found in the model function of the Estimator object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aritz/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "input_data = tf.feature_column.input_layer(features, my_feature_columns)\n",
    "input_data = tf.cast(input_data, tf.int32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "data = tf.cast(data, tf.float32)\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.8)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, 2]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=(2,)))\n",
    "logits = (tf.matmul(last, weight) + bias)\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('accuracy', accuracy[1])\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next operation is required in order to use reinitializable iterators but not for simple one-shot iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op_train = iterator.make_initializer(dataset_train)\n",
    "init_op_test = iterator.make_initializer(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_debug = tf.InteractiveSession()\n",
    "sess_debug.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason it seems to be necessary to add the following line though it wasn't in the original script. Otherwise I optain a FailedPreconditionError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_debug.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_debug = tf.summary.FileWriter(model_dir, sess_debug.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the **training** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "0th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[1 1 1 1 1 1 1 1 1 1] \n",
      "loss3.361319065093994 \n",
      "accuracy(0.0, 0.0)\n",
      "1th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.0, 0.5)\n",
      "2th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.5, 0.6666667)\n",
      "3th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.6666667, 0.75)\n",
      "4th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.75, 0.8)\n",
      "5th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.8, 0.8333333)\n",
      "6th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.8333333, 0.85714287)\n",
      "7th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss1.192092824453539e-08 \n",
      "accuracy(0.85714287, 0.875)\n",
      "8th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.875, 0.8888889)\n",
      "9th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.8888889, 0.9)\n",
      "10th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9, 0.90909094)\n",
      "11th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.90909094, 0.9166667)\n",
      "12th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9166667, 0.9230769)\n",
      "13th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9230769, 0.9285714)\n",
      "14th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9285714, 0.93333334)\n",
      "15th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.93333334, 0.9375)\n",
      "16th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9375, 0.9411765)\n",
      "17th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9411765, 0.9444444)\n",
      "18th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss1.192092824453539e-08 \n",
      "accuracy(0.9444444, 0.94736844)\n",
      "19th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.94736844, 0.95)\n",
      "20th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.95, 0.95238096)\n",
      "21th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.95238096, 0.95454544)\n",
      "22th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.95454544, 0.95652175)\n",
      "23th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.95652175, 0.9583333)\n",
      "24th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9583333, 0.96)\n",
      "25th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss1.192092824453539e-08 \n",
      "accuracy(0.96, 0.96153843)\n",
      "26th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.96153843, 0.962963)\n",
      "27th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.962963, 0.96428573)\n",
      "28th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss5.960462701182223e-08 \n",
      "accuracy(0.96428573, 0.9655172)\n",
      "29th batch\n",
      "labels[0 0 0 0 0 0 0 0 0 0] \n",
      "predicted_classes[0 0 0 0 0 0 0 0 0 0] \n",
      "loss0.0 \n",
      "accuracy(0.9655172, 0.96666664)\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    print('Epoch {}'.format(j))\n",
    "    # Initialize an iterator over the training dataset.\n",
    "    sess_debug.run([init_op_train])\n",
    "    for i in range(30):\n",
    "        print('{}th batch'.format(i))\n",
    "        labels_i, predicted_classes_i, loss_i, accuracy_i = sess_debug.run([labels, \n",
    "                                                                            predicted_classes, \n",
    "                                                                            loss, accuracy])\n",
    "        print('labels{} '.format(labels_i))\n",
    "        print('predicted_classes{} '.format(predicted_classes_i))\n",
    "        print('loss{} '.format(loss_i))\n",
    "        print('accuracy{}'.format(accuracy_i))\n",
    "        \n",
    "        sess_debug.run(train_op)\n",
    "        #if (i % 10 == 0):  \n",
    "        #    summary, acc = sess_debug.run([merged, accuracy])\n",
    "        #    print(\"Accuracy = {}\".format(acc[1]))\n",
    "        #    writer_debug.add_summary(summary, i)\n",
    "\n",
    "#    # Initialize an iterator over the testing dataset.\n",
    "#    sess_debug.run(init_op_test)\n",
    "#    for i in range(10):\n",
    "#        sess_debug.run(merged)\n",
    "#        summary, acc = sess_debug.run([merged, accuracy])\n",
    "#        print(\"Accuracy = {}\".format(acc[1]))\n",
    "#    #    writer_debug.add_summary(summary, 10*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_debug.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
