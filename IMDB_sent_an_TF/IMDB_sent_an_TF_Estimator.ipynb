{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB reviews: TensorFlow GloVe and LSTM with Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to perform sentiment analysis using TensorFlow. Most of the notebook is a variation of what was done on this blog:\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "This is an upgrade of the previous notebook (IMDB_sent_an_TF_basic_improved1) where I'm replacing the basic APIs by custom Estimator level APIs. For this I follow the indications of these tutorials coming from the official documentation:\n",
    "\n",
    "https://www.tensorflow.org/get_started/premade_estimators\n",
    "\n",
    "https://www.tensorflow.org/get_started/datasets_quickstart\n",
    "\n",
    "https://www.tensorflow.org/get_started/custom_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data exploration part (measuring the average number of words in the reviews) and the data preprocessing, turning texts into sequence of indexes corresponding the GloVes word embeddings, are done in another notebook called IMDB_sent_an_data_preprocessing. The variable created there are then loaded in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading matrices of embedding indexes and lists of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different **word embedding sizes**. The possibilities are 50, 100, 200, 300. We define the one we use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_size = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepr_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_data_preprocessing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = np.load(prepr_dir+'Saved_embeddings/idsMatrixTrain'+word_emb_size+'.npy')\n",
    "ids_test = np.load(prepr_dir+'Saved_embeddings/idsMatrixTest'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = ids_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the **labels** with and without **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"y_train_ord.txt\", \"rb\") as fp:\n",
    "    y_train_ord = pickle.load(fp)\n",
    "\n",
    "with open(prepr_dir+\"y_test_ord.txt\", \"rb\") as fp:\n",
    "    y_test_ord = pickle.load(fp)\n",
    "\n",
    "with open(prepr_dir+\"y_train.txt\", \"rb\") as fp:\n",
    "    y_train = pickle.load(fp)\n",
    "\n",
    "with open(prepr_dir+\"y_test.txt\", \"rb\") as fp:\n",
    "    y_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the **list of words in the GloVe table** and a numpy array containing the **GloVe look-up table**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"words_list.txt\", \"rb\") as fp:\n",
    "    words_list = pickle.load(fp)\n",
    "\n",
    "word_vectors = np.load(prepr_dir+'word_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a model using the Estimator APIs from TF. One of the advantage of this higher level of APIs is that some things done manualy when using TF basic APIs, are done automatically. There is no need to initialize variables for instance, or defining writers for TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the **input functions**. They are the objects which supply data for training, evaluating, and prediction to the model. They are using `tf.data.Dataset` objects which are one of the key tools of TF. These objects allow to access the data and manipulate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the body of the next function, it is important that the argument of the `shuffle` method is equal to the length of the whole training data set. See entry of the 11.07.18 of my journal for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trn = ids_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'Indexes':features}, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(len_trn).repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a numpy array of shape (#samples, 250)\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = {'Indexes':features}\n",
    "    else:\n",
    "        inputs = ({'Indexes':features}, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **feature columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = []\n",
    "my_feature_columns.append(tf.feature_column.numeric_column(key='Indexes', shape=max_seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **directory where to store the log files for TensorBoard** as well as the checkpoint file, the model files, and the graph file. These last files enable the notebook to **automatically store and restaure** previously trained models (as long as the architecture is the same in the old and the new notebook), as explained in this tutorial:\n",
    "\n",
    "https://www.tensorflow.org/guide/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_'+word_emb_size+'d/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define a custom estimator we need to define a **model function**. For this we mix the code of the notebook based only on basic TF APIs together with some parts of the script of this tutorial: https://www.tensorflow.org/get_started/custom_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    # Use `input_layer` to apply the feature columns.\n",
    "    input_data = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # The next line is required because tf.feature_column.input_layer\n",
    "    # outputs tf.float32 (whatever the input)\n",
    "    # and tf.nn.embedding_lookup requires\n",
    "    # tf.int32\n",
    "    input_data = tf.cast(input_data, tf.int32)\n",
    "    # Transform each index in a sentence into the associated vector\n",
    "    data = tf.nn.embedding_lookup(word_vectors, input_data)\n",
    "    # The following line is a fixe coming from this page:\n",
    "    # https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series/issues/2\n",
    "    # in order to prevent an error appearing next.\n",
    "    data = tf.cast(data, tf.float32)\n",
    "    # Next we define the LSTM\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(params['lstm_units'])\n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=params['keep_prob'])\n",
    "    value, _ = tf.nn.dynamic_rnn(lstm_cell, data, dtype=tf.float32)\n",
    "    # swaps the two first dimensions so it has dimensions [max_time, batch_size, cell.output_size]\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    # If I'm not mistaken the next cell slices the part of \n",
    "    # the output which corresponds to the last output of the lstm, \n",
    "    # or in other words the output corresponding to the \n",
    "    # last word for every sample (if I'm right we used \n",
    "    # 0 padding and cut everything which goes beyound 250 words, \n",
    "    # so technically it is the 250th output). \n",
    "    # My guess is that last has dimensions [batch_size, cell.output_size] \n",
    "    # which we can then use to do matrix multiplication \n",
    "    # with weight which has dimensions [cell.output_size, numClasses] \n",
    "    # (remember that cell.output_size=lstm_units).\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    # We apply an affine transformation to get the logits\n",
    "    #weight = tf.Variable(tf.truncated_normal([params['lstm_units'], params['n_classes']]))\n",
    "    #bias = tf.Variable(tf.constant(0.1, shape=[params['n_classes']]))\n",
    "    #logits = (tf.matmul(last, weight) + bias)\n",
    "    logits = tf.layers.dense(inputs=last, units=2)\n",
    "    # Maybe I could  replace this last part using tf.layers.dense:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "    \n",
    "    # The following lines are actually independent of the achitecture\n",
    "    # of the model.\n",
    "    \n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Compute loss\n",
    "    \n",
    "    # Note that because of this function, we have to\n",
    "    # provide ordinaly encoded labels and not one-hot-encoded\n",
    "    # labels, as explained on this page:\n",
    "    # https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define the **custom estimator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_service': None, '_log_step_count_steps': 100, '_evaluation_master': '', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f69a34faa20>, '_save_summary_steps': 100, '_task_id': 0, '_global_id_in_cluster': 0, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_is_chief': True, '_model_dir': '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/', '_tf_random_seed': None, '_master': '', '_save_checkpoints_steps': None}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        model_dir=model_dir,\n",
    "        params={\n",
    "            'feature_columns': my_feature_columns,\n",
    "            'n_classes': 2,\n",
    "            'lstm_units': 64,\n",
    "            'keep_prob': 0.7\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation of the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(100)\n",
    "train_steps = int(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can **train** our Estimator. Note that for the function `tf.losses.sparse_softmax_cross_entropy`, that we are using in the model function, requires the **label** to be **ordinaly encoded and not one-hot-encoded** as explained here:\n",
    "\n",
    "https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/aritz/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt-14003\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 14004 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt.\n",
      "INFO:tensorflow:step = 14003, loss = 0.18444294\n",
      "INFO:tensorflow:Saving checkpoints for 14006 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17752373.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f69a34fa5f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(features=ids_train, labels=y_train_ord, batch_size=batch_size),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-21-21:44:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt-14006\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-21-21:45:10\n",
      "INFO:tensorflow:Saving dict for global step 14006: accuracy = 0.84184635, global_step = 14006, loss = 0.48276785\n"
     ]
    }
   ],
   "source": [
    "eval_test = classifier.evaluate(input_fn=lambda:eval_input_fn(features=ids_test,\n",
    "                                                                    labels=y_test_ord,\n",
    "                                                                    batch_size=batch_size)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we print the **accuracy of the model on the test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.84184635, 'global_step': 14006, 'loss': 0.48276785}\n"
     ]
    }
   ],
   "source": [
    "print(eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will implement a loop which will train the model as long as the quality of the prediction keeps improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(100)\n",
    "train_steps = int(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_i = 0\n",
    "acc_iplus1 = 0\n",
    "acc_iplus2 = eval_test['accuracy']\n",
    "num_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt-12000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 12001 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.116583526, step = 12000\n",
      "INFO:tensorflow:global_step/sec: 5.52669\n",
      "INFO:tensorflow:loss = 0.24673532, step = 12100 (18.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.42818\n",
      "INFO:tensorflow:loss = 0.1551571, step = 12200 (18.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.73969\n",
      "INFO:tensorflow:loss = 0.15166456, step = 12300 (17.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.5761\n",
      "INFO:tensorflow:loss = 0.34310123, step = 12400 (15.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50861\n",
      "INFO:tensorflow:loss = 0.11865516, step = 12500 (15.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46593\n",
      "INFO:tensorflow:loss = 0.3100912, step = 12600 (15.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.84718\n",
      "INFO:tensorflow:loss = 0.13938998, step = 12700 (17.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.45558\n",
      "INFO:tensorflow:loss = 0.15073161, step = 12800 (15.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.60011\n",
      "INFO:tensorflow:loss = 0.24637079, step = 12900 (15.151 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13000 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17228603.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-15-11:38:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt-13000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-15-11:39:15\n",
      "INFO:tensorflow:Saving dict for global step 13000: accuracy = 0.84956604, global_step = 13000, loss = 0.42755738\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt-13000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 13001 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.1351165, step = 13000\n",
      "INFO:tensorflow:global_step/sec: 5.78828\n",
      "INFO:tensorflow:loss = 0.11435699, step = 13100 (17.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.47174\n",
      "INFO:tensorflow:loss = 0.13042718, step = 13200 (15.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.45931\n",
      "INFO:tensorflow:loss = 0.10244565, step = 13300 (15.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97935\n",
      "INFO:tensorflow:loss = 0.04903977, step = 13400 (16.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.16554\n",
      "INFO:tensorflow:loss = 0.23376682, step = 13500 (16.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.52103\n",
      "INFO:tensorflow:loss = 0.1585369, step = 13600 (15.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.12519\n",
      "INFO:tensorflow:loss = 0.21807069, step = 13700 (16.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14174\n",
      "INFO:tensorflow:loss = 0.16492325, step = 13800 (16.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24359\n",
      "INFO:tensorflow:loss = 0.118033655, step = 13900 (16.016 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14000 into /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1796388.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-15-11:43:08\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/Saved_models/Basic_LSTM_100d/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-15-11:43:31\n",
      "INFO:tensorflow:Saving dict for global step 14000: accuracy = 0.8447662, global_step = 14000, loss = 0.47048092\n"
     ]
    }
   ],
   "source": [
    "while ((acc_iplus2 > acc_iplus1) or (acc_iplus2 > acc_i)):\n",
    "    num_iter = num_iter + 1\n",
    "    classifier.train(\n",
    "    input_fn=lambda:train_input_fn(features=ids_train, labels=y_train_ord, batch_size=batch_size),\n",
    "    steps=train_steps)\n",
    "    acc_i = acc_iplus1\n",
    "    acc_iplus1 = acc_iplus2\n",
    "    eval_test = classifier.evaluate(input_fn=lambda:eval_input_fn(features=ids_test,\n",
    "                                                                    labels=y_test_ord,\n",
    "                                                                    batch_size=batch_size)\n",
    "                                     )\n",
    "    acc_iplus2 = eval_test['accuracy']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83672655\n",
      "0.8407664\n",
      "0.83468664\n"
     ]
    }
   ],
   "source": [
    "print(acc_i)\n",
    "print(acc_iplus1)\n",
    "print(acc_iplus2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track the progress of the model on TensorBoard it is enough to enter \"tensorboard --logdir (...)\" in a terminal with \"(...)\" replaced by the name of the directory where the event files are saved, and visiting http://localhost:6006/ with a browser.\n",
    "\n",
    "With custom estimators it is enough to include lines like\n",
    "`tf.summary.scalar('loss', loss)`\n",
    "in the model function in order to track the quantities we are interested in. An event file for TensorBoard will be updated every 100 steps during the training with the `train` method (at least it is what I observe from my empirical experience), and a single event file measuring the state of the tracked variable will be written when calling the `evaluate`method. There is no need to define a writer with `tf.summary.FileWriter`.\n",
    "\n",
    "**Caveat emptor:** This being said, I often ran into problems with TensorBoard. No files were being written, or only during the evaluation phase. Currently it seems to work as I described above, but I cannot garantee that what I described is absolutely true. I don't know yet how to specify the behaviour of an Estimator object when it comes to TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells I use TF basic APIs to access directly what is happening when I call the train method of my Estimator object. This allows me to understand source of errors and warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(100)\n",
    "lstm_units = int(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining **two datasets**. One for the training and one for the testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_input_fn(features=ids_train, labels=y_train_ord, batch_size=batch_size)\n",
    "dataset_test = eval_input_fn(features=ids_test, labels=y_test_ord, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **reinitializable iterator**. Unlike One-shot iterators, they alow to switch from one dataset to another one. As explained here:\n",
    "https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator, \n",
    "\"A reinitializable iterator is defined by its structure. We could use the\n",
    " `output_types` and `output_shapes` properties of either `dataset_train`\n",
    " or `dataset_test` here, because they are compatible.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                           dataset_train.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the model itself (this part is similar to what is found in the model function of the Estimator object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.feature_column.input_layer(features, my_feature_columns)\n",
    "input_data = tf.cast(input_data, tf.int32)\n",
    "data = tf.nn.embedding_lookup(word_vectors, input_data)\n",
    "data = tf.cast(data, tf.float32)\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
    "lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=0.8)\n",
    "value, _ = tf.nn.dynamic_rnn(lstm_cell, data, dtype=tf.float32)\n",
    "value = tf.transpose(value, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#weight = tf.Variable(tf.truncated_normal([lstm_units, 2]))\n",
    "#bias = tf.Variable(tf.constant(0.1, shape=(2,)))\n",
    "#logits = (tf.matmul(last, weight) + bias)\n",
    "logits = tf.layers.dense(inputs=last, units=2)\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('accuracy', accuracy[1])\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next operation is required in order to use reinitializable iterators but not for simple one-shot iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op_train = iterator.make_initializer(dataset_train)\n",
    "init_op_test = iterator.make_initializer(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_debug = tf.InteractiveSession()\n",
    "sess_debug.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason it seems to be necessary to add the following line though it wasn't in the original script. Otherwise I optain a FailedPreconditionError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_debug.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_train = tf.summary.FileWriter(model_dir+'Basic_log/Plot_train/', sess_debug.graph)\n",
    "writer_test = tf.summary.FileWriter(model_dir+'Basic_log/Plot_test/', sess_debug.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the **training** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(15):\n",
    "    print('Epoch {}'.format(j))\n",
    "    # Initialize an iterator over the training dataset.\n",
    "    sess_debug.run([init_op_train])\n",
    "    print('Training')\n",
    "    for i in range(100):\n",
    "        sess_debug.run(train_op)\n",
    "        if (i % 50 == 0):  \n",
    "            summary, acc = sess_debug.run([merged, accuracy])\n",
    "            print(\"Accuracy = {}\".format(acc[1]))\n",
    "            writer_train.add_summary(summary, j*100)\n",
    "    \n",
    "    print('Testing')\n",
    "    # Initialize an iterator over the testing dataset.\n",
    "    sess_debug.run(init_op_test)\n",
    "    sess_debug.run(merged)\n",
    "    summary, acc = sess_debug.run([merged, accuracy])\n",
    "    print(\"Accuracy = {}\".format(acc[1]))\n",
    "    writer_test.add_summary(summary, j*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_debug.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
