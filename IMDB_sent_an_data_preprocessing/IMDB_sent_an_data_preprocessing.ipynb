{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB reviews: data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code of notebook comes from this page\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "I realized that I was repeating in all my notebooks (TensorFlow, Keras, Pytorch) the same data inspection and data preprocessing, and thought it would be better to put it in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "from matplotlib import pyplot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different **word embedding sizes**. The possibilities are 50, 100, 200, 300. We define the one we use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_size = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_file_name = 'glove.6B/glove.6B.' + word_emb_size + 'd.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the next cell comes from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "emb_df = pd.read_table(emb_file_name, sep=\" \",\n",
    "                       index_col=0, header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the Pandas data frame into one list `words_list` containing the words (the indexes from the data frame) and one numpy array `word_vectors` containing the corresponding vectors. This last data frame will play the role of our **look-up table** later when we define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = list(emb_df.index)\n",
    "word_vectors = emb_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words_list.txt\", \"wb\") as fp:\n",
    "    pickle.dump(words_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_vectors', word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I commented this part because it is useless for the real task. It only serves a pedagogic purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseball_idx = words_list.index('baseball')\n",
    "#word_vectors[baseball_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seq_len = 10 #Maximum length of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_sentence = np.zeros((max_seq_len), dtype='int32')\n",
    "#first_sentence[0] = words_list.index(\"i\")\n",
    "#first_sentence[1] = words_list.index(\"thought\")\n",
    "#first_sentence[2] = words_list.index(\"the\")\n",
    "#first_sentence[3] = words_list.index(\"movie\")\n",
    "#first_sentence[4] = words_list.index(\"was\")\n",
    "#first_sentence[5] = words_list.index(\"incredible\")\n",
    "#first_sentence[6] = words_list.index(\"and\")\n",
    "#first_sentence[7] = words_list.index(\"inspiring\")\n",
    "##first_sentence[8] and first_sentence[9] are going to be 0\n",
    "#print(first_sentence.shape)\n",
    "#print(first_sentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    print(tf.nn.embedding_lookup(word_vectors,first_sentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are looking for the threshold we should take as maximum length of a text. In our implementation, we want to use LSTM, and feed it with inputs of same length. A priori, the IMDB reviews have different lengths, so what we do is to cut all texts which are longer than a given threshold (that we determine here) and use zero padding for the ones which are smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_POS = TRAIN + 'pos/'\n",
    "TRAIN_NEG = TRAIN + 'neg/'\n",
    "pos_files_trn = [TRAIN_POS + f for f in listdir(TRAIN_POS) if isfile(join(TRAIN_POS, f))]\n",
    "neg_files_trn = [TRAIN_NEG + f for f in listdir(TRAIN_NEG) if isfile(join(TRAIN_NEG, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_POS = TEST + 'pos/'\n",
    "TEST_NEG = TEST + 'neg/'\n",
    "pos_files_test = [TEST_POS + f for f in listdir(TEST_POS) if isfile(join(TEST_POS, f))]\n",
    "neg_files_test = [TEST_NEG + f for f in listdir(TEST_NEG) if isfile(join(TEST_NEG, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a good threshold, we determine the average number of words in one sample of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "n_words = []\n",
    "for pf in pos_files_trn:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        n_words.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in neg_files_test:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        n_words.append(counter)  \n",
    "print('Negative files finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files_trn = len(pos_files_trn) + len(neg_files_trn)\n",
    "n_files_test = len(pos_files_test) + len(neg_files_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely it seems that there aren't exactly 12500 files in the folders indicated below, as it is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n",
      "12501\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_files_trn))\n",
    "print(len(neg_files_trn))\n",
    "print(len(pos_files_test))\n",
    "print(len(neg_files_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 25002\n",
      "The total number of words in the files is 5809599\n",
      "The average number of words in the files is 232.37466501339946\n"
     ]
    }
   ],
   "source": [
    "print('The total number of files is', n_files_trn)\n",
    "print('The total number of words in the files is', sum(n_words))\n",
    "print('The average number of words in the files is', sum(n_words)/len(n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an histogram of the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(n_words, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, 250 words seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"max_seq_len.txt\", \"wb\") as fp:\n",
    "    pickle.dump(max_seq_len, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we map words to indexes using our words_list variable.\n",
    "\n",
    "**Question**: It is not entirely clear to me, how the unknown tokens are dealt with. From the code, a new token is assigned the index 399'999. But this corresponds to a word in the GloVe embedding data frame (\"sandberger\"). I guess that as this word is uncommun, it might serve the purpose of designating unknown words. Even stranger to me is the way padding is used: the author of the code I copied (https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow) uses 0 padding. But in our system, 0 corresponds to \"the\" and will be treated as a real word by the lstm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do it for a specific file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the content of the file (i.e. the real text).  (I commented out this part because it is useless for the real task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = pos_files_trn[3] #Can use any valid index (not just 3)\n",
    "#with open(fname) as f:\n",
    "#    for lines in f:\n",
    "#        print(lines)\n",
    "#        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters. In more details, as explained on this page:\n",
    "\n",
    "https://www.regular-expressions.info/quickstart.html\n",
    "\n",
    "`[A-Za-z0-9 ]` is a regular expression matching (once) any alphanumeric character or the space, `[^A-Za-z0-9 ]` matches (once) any character which is not one of those (because of the `^`), and `[^A-Za-z0-9 ]+`, matches the class of non-alphanumeric (or space) characters one or more times (because of the `+`). In the line\n",
    "\n",
    "`re.sub(strip_special_chars, \"\", string.lower())`\n",
    "\n",
    "this this expression is removed (replaced by `\"\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def clean_sentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the given file to a list of indexes where each indexe corresponds to a word, according to the list `words_list`. (I commented out this part because it is useless for the real task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_file = np.zeros((max_seq_len), dtype='int32')\n",
    "#with open(fname) as f:\n",
    "#    idx_counter = 0\n",
    "#    line=f.readline()\n",
    "#    cleaned_line = clean_sentences(line)\n",
    "#    split = cleaned_line.split()\n",
    "#    for word in split:\n",
    "#        try:\n",
    "#            first_file[idx_counter] = words_list.index(word)\n",
    "#        except ValueError:\n",
    "#            first_file[idx_counter] = 399999 #Vector for unknown words\n",
    "#        idx_counter = idx_counter + 1\n",
    "#first_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids_train will be a matrix, which contains for each file of the training set (first the positive ones, then the negative ones) a row where the columns contain the indices corresponding to the words of the sample file.\n",
    "\n",
    "The whole computation of the transformation of each text file into a list of indices takes time and needs to be performed only once. The result is saved after the first time and then reloaded for all the other executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_train = np.zeros((n_files_trn, max_seq_len), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_counter = 0\n",
    "#for pf in pos_files_trn:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in neg_files_trn:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "#np.save('Saved_embeddings/idsMatrixTrain'+word_emb_size, ids_train)\n",
    "\n",
    "ids_train = np.load('Saved_embeddings/idsMatrixTrain'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_test = np.zeros((n_files_test, max_seq_len), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_counter = 0\n",
    "#for pf in pos_files_test:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nf in neg_files_test:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('Saved_embeddings/idsMatrixTest'+word_emb_size, ids_test)\n",
    "\n",
    "ids_test = np.load('Saved_embeddings/idsMatrixTest'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also create the **labels** with **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative), as done in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_pos_trn = len(pos_files_trn)\n",
    "len_neg_trn = len(neg_files_trn)\n",
    "y_train = [[1,0] for i in range(len_pos_trn)] + [[0,1] for i in range(len_neg_trn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_pos_test = len(pos_files_test)\n",
    "len_neg_test = len(neg_files_test)\n",
    "y_test = [[1,0] for i in range(len_pos_test)] + [[0,1] for i in range(len_neg_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_trn = len_pos_trn + len_neg_trn\n",
    "len_test = len_pos_test + len_neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some premade estimators do not accept one-hot-encoding of the labels as explained here:https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "\n",
    "For this reason I create a function which converts a list containing one-hot-incoded labels into a list containing **ordinal encoded** labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_2_ordinal(onehot):\n",
    "    n_classes = len(onehot[0])\n",
    "    ordinal = []\n",
    "    for i in range(len(onehot)):\n",
    "        for j in range(n_classes):\n",
    "            if onehot[i][j]==1:\n",
    "                ordinal.append(j)\n",
    "    return(ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ord = onehot_2_ordinal(y_train)\n",
    "y_test_ord = onehot_2_ordinal(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save it to access it from the other notebooks, following this Stack Overflow page: https://stackoverflow.com/questions/27745500/how-to-save-a-list-to-a-file-and-read-it-as-a-list-type#27745539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_train.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_train, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_train_ord.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_train_ord, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_test_ord.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_test_ord, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
