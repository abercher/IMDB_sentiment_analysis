{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing previous architecture with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started my project from this blog which presents an implementation based on TensorFlow basic APIs.\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "I will reproduce in this section the architecture presented there, i.e. a simple LSTM on top of GloVes embeddings, and reuse some of the data preprocessing, but using Keras APIs instead, to see how the two compares in terms of complexity of the code and performances.\n",
    "\n",
    "The beginning is similar to what was done before, with TensorFlow. We transform the texts into vectors of indices to be used with GloVe look-up table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data exploration part (measuring the average number of words in the reviews) and the data preprocessing, turning texts into sequence of indexes corresponding the GloVes word embeddings, are done in another notebook called IMDB_sent_an_data_preprocessing. The variable created there are then loaded in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading matrices of embedding indexes and lists of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different **word embedding sizes**. The possibilities are 50, 100, 200, 300. We define the one we use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_size = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepr_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_data_preprocessing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = np.load(prepr_dir+'Saved_embeddings/idsMatrixTrain'+word_emb_size+'.npy')\n",
    "ids_test = np.load(prepr_dir+'Saved_embeddings/idsMatrixTest'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = ids_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the **labels** with and without **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"y_train_ord.txt\", \"rb\") as fp:\n",
    "    y_train_ord = pickle.load(fp)\n",
    "\n",
    "with open(prepr_dir+\"y_test_ord.txt\", \"rb\") as fp:\n",
    "    y_test_ord = pickle.load(fp)\n",
    "\n",
    "with open(prepr_dir+\"y_train.txt\", \"rb\") as fp:\n",
    "    y_train = pickle.load(fp)\n",
    "\n",
    "with open(prepr_dir+\"y_test.txt\", \"rb\") as fp:\n",
    "    y_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the **list of words in the GloVe table** and a numpy array containing the **GloVe look-up table**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"words_list.txt\", \"rb\") as fp:\n",
    "    words_list = pickle.load(fp)\n",
    "\n",
    "word_vectors = np.load(prepr_dir+'word_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a little doubt about the `input_dim` argument of `Embedding`: I think that in our case it is equal to the length of `words_list` above because it is described on the official page as the length of the vocabulary:\n",
    "\n",
    "but in this tutorial\n",
    "\n",
    "they add one to the length of the vocabulary. I guess it is because in the later case, they have to add one token for the padding symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=len(words_list),\n",
    "                    output_dim=int(word_emb_size),\n",
    "                    weights=[word_vectors],\n",
    "                    input_length=max_seq_len,\n",
    "                    trainable=False,\n",
    "                    mask_zero=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 64\n",
    "keep_prob = 0.7\n",
    "model.add(LSTM(lstm_units, dropout=1-keep_prob, recurrent_dropout=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25002 samples, validate on 25001 samples\n",
      "Epoch 1/1\n",
      "25002/25002 [==============================] - 134s 5ms/step - loss: 0.6174 - acc: 0.6546 - val_loss: 0.5109 - val_acc: 0.7573\n",
      "25001/25001 [==============================] - 34s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(ids_train, y_train_ord,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(ids_test, y_test_ord))\n",
    "score, acc = model.evaluate(ids_test, y_test_ord,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7572897064757821"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
