{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB reviews: Pytorch implementation of LSTM on top of GloVes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will try to implement with Pytorch the architecture that I found on this blog:\n",
    "\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "which is basically a LSTM on the top of an embedding layer using GloVe pretrained embeddings. I will reuse part of the code presented in the page mentioned above for the data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "from matplotlib import pyplot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data exploration part (measuring the average number of words in the reviews) and the data preprocessing, turning texts into sequence of indexes corresponding the GloVes word embeddings, are done in another notebook called IMDB_sent_an_data_preprocessing. The variable created there are then loaded in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading matrices of embedding indexes and lists of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different **word embedding sizes**. The possibilities are 50, 100, 200, 300. We define the one we use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_size = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepr_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_data_preprocessing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = np.load(prepr_dir+'Saved_embeddings/idsMatrixTrain'+word_emb_size+'.npy')\n",
    "ids_test = np.load(prepr_dir+'Saved_embeddings/idsMatrixTest'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the **labels** with and without **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"y_train_ord.txt\", \"rb\") as fp:\n",
    "    y_train_ord = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"y_test_ord.txt\", \"rb\") as fp:\n",
    "    y_test_ord = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"y_train.txt\", \"rb\") as fp:\n",
    "    y_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"y_test.txt\", \"rb\") as fp:\n",
    "    y_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the **list of words in the GloVe table** and a numpy array containing the **GloVe look-up table**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepr_dir+\"words_list.txt\", \"rb\") as fp:\n",
    "    words_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load(prepr_dir+'word_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, I define a `nn.Module` model which does almost all the transformation we want, i.e. from the embedding of the indexes to the making of the logits. I copied some of the code I found in this tutorial:\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "The main difficulty when writing these models, is to get the dimensions of all the tensors right. In particular, using batches with DataLoader objects, adds one dimension.\n",
    "\n",
    "Note that using `nn.CrossEntropyLoss()` as loss function is equivalent to combining `nn.LogSoftmax(dim=1)` as last layer of the model together with `nn.NLLLoss()` as loss function. I choose the first option here as it looks more similar to the implementation using TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeLSTM(nn.Module):\n",
    "    def __init__(self, weights_matrix, keep_prob, n_lstm_units):\n",
    "        super(GloVeLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weights_matrix)\n",
    "        _, self.emb_dim = weights_matrix.shape\n",
    "        self.embeddings.weight.requires_grad=False\n",
    "        self.n_lstm_units = n_lstm_units\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.lstm = nn.LSTM(input_size=self.emb_dim,\n",
    "                            hidden_size=n_lstm_units,\n",
    "                            dropout=1-keep_prob,\n",
    "                            batch_first=True).double()\n",
    "        self.hidden2bin = nn.Linear(n_lstm_units, 2).double()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #Before using the LSTM, we need to clean\n",
    "        #its hidden and state variable in order\n",
    "        #to prevent the previous review to influence\n",
    "        #the output for the new review.\n",
    "        bs = inp.shape[0]\n",
    "        self.hidden = self.init_hidden(bs=bs)\n",
    "        emb_vect = self.embeddings(inp)\n",
    "        #We only care about the output of the LSTM for\n",
    "        #the last word of the sentence (which is the \n",
    "        #first entry of the self.hidden)\n",
    "        _, self.hidden = self.lstm(\n",
    "            emb_vect, self.hidden)\n",
    "        logits = self.hidden2bin(self.hidden[0].view(\n",
    "            bs, self.n_lstm_units))\n",
    "        return(logits)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, bs=1):\n",
    "        hidden = (torch.zeros(1, bs, self.n_lstm_units),\n",
    "                  torch.zeros(1, bs, self.n_lstm_units))\n",
    "        hidden = (torch.tensor(hidden[0], dtype=torch.float64),\n",
    "                  torch.tensor(hidden[1], dtype=torch.float64))\n",
    "        return(hidden)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, I make sure that this model works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "classifier = GloVeLSTM(weights_matrix=torch.from_numpy(word_vectors),\n",
    "                         keep_prob=0.7,\n",
    "                         n_lstm_units=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all input sequences which have a lot of zeros at the end, produce the same logits (without training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n",
      "[   116    285    998      7    530    439    413 399999  27015]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 5.8547, -0.0628]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n",
      "[   13    37 15890     0     0     0     0     0     0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.2382,  8.7623]], dtype=torch.float64)\n",
      "[   0 3121    3    0 5317 1468 1351    0 8973]\n",
      "tensor(1.00000e-02 *\n",
      "       [[-6.5206, -6.4991]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0713,  8.8323]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(ids_train[i][-10:-1])\n",
    "    print(classifier(torch.from_numpy(np.int64(ids_train[i])).view(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **loss function**. From this Quora page: https://www.quora.com/What-are-the-differences-between-maximum-likelihood-and-cross-entropy-as-a-loss-function it seems that the negative log likelihood loss and the binary cross-entropy loss are the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimizer, one has to give only the parameters which have `requires_grad=True`, as explained here:\n",
    "https://discuss.pytorch.org/t/freeze-the-learnable-parameters-of-resnet-and-attach-it-to-a-new-network/949/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, classifier.parameters())\n",
    "optimizer = torch.optim.Adagrad(parameters, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try to train the model on a single input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.zero_grad()\n",
    "inp = torch.from_numpy(np.int64(ids_train[0]))\n",
    "y = y_train_ord[0:1]\n",
    "logits = classifier(inp.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fcn(logits, torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7327, dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our custom Dataset. Note that there exists an official dataset for IMDB:\n",
    "\n",
    "https://torchtext.readthedocs.io/en/latest/datasets.html#imdb\n",
    "\n",
    "but I won't use it since my goal is to get comfortable with tools that I could later use for any set of data.\n",
    "\n",
    "**Edit**: Actually, it looks like there is a cleaner way to define the Dataset, since we have tensors, namely use `my_dataset = data_utils.TensorDataset(ids_train, y_train_ord)`, but in my case, I obtain errors when I try to use it (`TypeError: 'int' object is not callable`), so I will stick with my custom Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIMDBDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ids_matrix, y_ord):\n",
    "        self.ids_matrix = ids_matrix\n",
    "        self.y_ord = y_ord\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(self.ids_matrix.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return((self.ids_matrix[idx], self.y_ord[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyIMDBDataset(ids_train, y_train_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Dataset` object outputs objects having the type that one would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when one use the `DataLoader` object, the objects that we want to iterate over are **automatically wrapped inside tensors**, as one can see in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 4.1000e+01,  1.7750e+03,  3.7000e+01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.1000e+01,  1.9909e+05,  2.1370e+03,  ...,  1.1600e+02,\n",
      "          2.8500e+02,  6.4160e+03],\n",
      "        [ 2.7900e+02,  2.8780e+03,  9.6000e+01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 4.1000e+01,  1.5000e+01,  1.9100e+02,  ...,  3.9040e+03,\n",
      "          1.0000e+02,  8.1000e+01],\n",
      "        [ 5.3000e+01,  6.4000e+01,  1.5220e+03,  ...,  1.0800e+03,\n",
      "          3.7800e+02,  3.9000e+01],\n",
      "        [ 4.1000e+01,  8.3500e+02,  3.7000e+01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], dtype=torch.int32)\n",
      "torch.Size([100, 250])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([ 1,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  0,\n",
      "         0,  0,  1,  0,  1,  1,  1,  1,  0,  1,  1,  0,  1,  0,\n",
      "         1,  1,  1,  1,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,\n",
      "         1,  1,  0,  1,  0,  1,  1,  0,  0,  1,  0,  1,  1,  0,\n",
      "         0,  1,  1,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,\n",
      "         1,  1,  0,  1,  1,  1,  0,  0,  1,  1,  1,  0,  1,  0,\n",
      "         1,  0,  0,  0,  0,  1,  1,  1,  0,  1,  0,  0,  0,  0,\n",
      "         1,  0])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    if i>0:\n",
    "        break\n",
    "    print(i)\n",
    "    inp, label = data\n",
    "    print(type(inp))\n",
    "    print(inp)\n",
    "    print(inp.shape)\n",
    "    print(type(label))\n",
    "    print(label)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Dataset and DataLoader for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyIMDBDataset(ids_test, y_test_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the following cell is a variation of what I found on this page https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html (for the training) and on this page https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html (for the testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inp, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # put the inputs into a torch.tensor\n",
    "        inp = torch.from_numpy(np.int64(inp))\n",
    "        logits = classifier(inp)\n",
    "        loss = loss_fcn(logits, torch.tensor(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # print the accuracy on the test set\n",
    "    if epoch > 5:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                input_test, labels = data\n",
    "                input_test = torch.tensor(input_test, dtype = torch.int64)\n",
    "                logits = classifier(input_test)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the test set: %05.3f %%' % (\n",
    "            100 * correct / total))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reuse the code found on this page: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 63.697 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        input_test, labels = data\n",
    "        input_test = torch.tensor(input_test, dtype = torch.int64)\n",
    "        logits = classifier(input_test)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test set: %05.3f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing components of the model step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells we try to find sources of errors. For this we start by creating some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_toy = 0.7\n",
    "n_lstm_units_toy = 7\n",
    "len_vocab_toy = 14\n",
    "emb_size_toy = 4\n",
    "seq_len_toy = 3\n",
    "weights_matrix_toy = torch.from_numpy(np.random.random([len_vocab_toy, emb_size_toy]))\n",
    "batch_size_toy = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_toy_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_toy = np.random.randint(low=0, high=14, size = seq_len_toy*ids_toy_len)\n",
    "ids_toy.shape = (ids_toy_len, seq_len_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_toy = ids_toy.astype(np.int32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_toy = [int(round(np.random.random_sample(1)[0])) for i in range(ids_toy_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_toy = (torch.zeros(1, batch_size_toy, n_lstm_units_toy),\n",
    "                torch.zeros(1, batch_size_toy, n_lstm_units_toy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_toy = (torch.tensor(hidden_toy[0], dtype=torch.float64),\n",
    "              torch.tensor(hidden_toy[1], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_toy = nn.Embedding.from_pretrained(weights_matrix_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, emb_dim_toy = weights_matrix_toy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "lstm_toy = nn.LSTM(input_size=emb_dim_toy,\n",
    "                   hidden_size=n_lstm_units_toy,\n",
    "                   dropout=1-keep_prob_toy,\n",
    "                   batch_first=True).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden2bin_toy = nn.Linear(n_lstm_units_toy, 2).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logsoft_toy = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn_toy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try to reproduce what the `forward function` would do on our toy data, with this setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_toy = torch.from_numpy(np.int64(ids_toy[:batch_size_toy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 12,   6,   9],\n",
      "        [  8,  12,  11]])\n"
     ]
    }
   ],
   "source": [
    "print(inp_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vect_toy = embeddings_toy(inp_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, hidden_toy = lstm_toy(emb_vect_toy, hidden_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 7])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_toy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_toy = hidden2bin_toy(hidden_toy[0].view(hidden_toy[0].shape[1],hidden_toy[0].shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_toy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([y_toy[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6919, dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fcn_toy(logits_toy, torch.tensor(y_toy[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_toy = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_toy = (torch.zeros(1, batch_size_toy, n_lstm_units_toy),\n",
    "                torch.zeros(1, batch_size_toy, n_lstm_units_toy))\n",
    "hidden_toy = (torch.tensor(hidden_toy[0], dtype=torch.float64),\n",
    "              torch.tensor(hidden_toy[1], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset_toy = MyIMDBDataset(ids_toy, y_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_toy = DataLoader(my_dataset_toy, batch_size=batch_size_toy, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([5, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader_toy, 0):\n",
    "    if i>0:\n",
    "        break\n",
    "    inp, labels = data\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    inp = torch.tensor(inp, dtype=torch.int64)\n",
    "    emb_vect_toy = embeddings_toy(inp)\n",
    "    print(type(emb_vect_toy))\n",
    "    print(emb_vect_toy.shape)\n",
    "    _, hidden_toy = lstm_toy(\n",
    "            emb_vect_toy, hidden_toy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
