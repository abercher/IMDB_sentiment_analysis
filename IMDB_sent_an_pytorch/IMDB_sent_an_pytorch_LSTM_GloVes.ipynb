{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB reviews: Pytorch implementation of LSTM on top of GloVes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will try to implement with Pytorch the architecture that I found on this blog:\n",
    "\n",
    "https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "\n",
    "which is basically a LSTM on the top of an embedding layer using GloVe pretrained embeddings. I will reuse part of the code presented in the page mentioned above for the data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained embeddins from GloVe can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different **word embedding sizes**. The possibilities are 50, 100, 200, 300. We define the one we use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_size = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_file_name = '../IMDB_sent_an_TF/glove.6B/glove.6B.' + word_emb_size + 'd.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the next cell comes from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "emb_df = pd.read_table(emb_file_name, sep=\" \",\n",
    "                       index_col=0, header=None, quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we transform the Pandas data frame into one list `words_list` containing the words (the indexes from the data frame) and one numpy array `word_vectors` containing the corresponding vectors. This last data frame will play the role of our **look-up table** later when we define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = list(emb_df.index)\n",
    "word_vectors = emb_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are looking for the threshold we should take as maximum length of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb/\"\n",
    "#PATH = \"/home/aritz/Documents/CS_Programming_Machine_Learning/Machine_learning_and_AI/Online_courses/Fast_AI/fastai/courses/dl1/data/aclImdb_sample2/\"\n",
    "TRAIN = PATH+'train/'\n",
    "TEST = PATH+'test/'\n",
    "\n",
    "TRAIN_POS = TRAIN + 'pos/'\n",
    "TRAIN_NEG = TRAIN + 'neg/'\n",
    "pos_files_trn = [TRAIN_POS + f for f in listdir(TRAIN_POS) if isfile(join(TRAIN_POS, f))]\n",
    "neg_files_trn = [TRAIN_NEG + f for f in listdir(TRAIN_NEG) if isfile(join(TRAIN_NEG, f))]\n",
    "\n",
    "TEST_POS = TEST + 'pos/'\n",
    "TEST_NEG = TEST + 'neg/'\n",
    "pos_files_test = [TEST_POS + f for f in listdir(TEST_POS) if isfile(join(TEST_POS, f))]\n",
    "neg_files_test = [TEST_NEG + f for f in listdir(TEST_NEG) if isfile(join(TEST_NEG, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine the average number of words in one sample of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "n_words = []\n",
    "for pf in pos_files_trn:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        n_words.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in neg_files_test:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        n_words.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "n_files_trn = len(pos_files_trn) + len(neg_files_trn)\n",
    "n_files_test = len(pos_files_test) + len(neg_files_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely it seems that there aren't exactly 12500 files in the folders indicated below, as it is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n",
      "12501\n",
      "12500\n",
      "The total number of files is 25002\n",
      "The total number of words in the files is 5809599\n",
      "The average number of words in the files is 232.37466501339946\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_files_trn))\n",
    "print(len(neg_files_trn))\n",
    "print(len(pos_files_test))\n",
    "print(len(neg_files_test))\n",
    "\n",
    "print('The total number of files is', n_files_trn)\n",
    "print('The total number of words in the files is', sum(n_words))\n",
    "print('The average number of words in the files is', sum(n_words)/len(n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot an histogram of the number of words in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(n_words, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, 250 words seems a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we map words to indexes using our words_list variable.\n",
    "\n",
    "**Question**: It is not entirely clear to me, how the unknown tokens are dealt with. From the code, a new token is assigned the index 399'999. But this corresponds to a word in the GloVe embedding data frame (\"sandberger\"). I guess that as this word is uncommun, it might serve the purpose of designating unknown words. Even stranger to me is the way padding is used: the author of the code I copied (https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow) uses 0 padding. But in our system, 0 corresponds to \"the\" and will be treated as a real word by the lstm.\n",
    "\n",
    "First we do it for a specific file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the content of the file (i.e. the real text).  (I commented out this part because it is useless for the real task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = pos_files_trn[3] #Can use any valid index (not just 3)\n",
    "#with open(fname) as f:\n",
    "#    for lines in f:\n",
    "#        print(lines)\n",
    "#        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the given file to a list of indexes where each indexe corresponds to a word, according to the list `words_list`. (I commented out this part because it is useless for the real task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_file = np.zeros((max_seq_len), dtype='int32')\n",
    "#with open(fname) as f:\n",
    "#    idx_counter = 0\n",
    "#    line=f.readline()\n",
    "#    cleaned_line = clean_sentences(line)\n",
    "#    split = cleaned_line.split()\n",
    "#    for word in split:\n",
    "#        try:\n",
    "#            first_file[idx_counter] = words_list.index(word)\n",
    "#        except ValueError:\n",
    "#            first_file[idx_counter] = 399999 #Vector for unknown words\n",
    "#        idx_counter = idx_counter + 1\n",
    "#first_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dir = '/home/aritz/Documents/CS_Programming_Machine_Learning/Projects/IMDB_sentiment_analysis/IMDB_sent_an_TF/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ids_train will be a matrix, which contains for each file of the training set (first the positive ones, then the negative ones) a row where the columns contain the indices corresponding to the words of the sample file.\n",
    "\n",
    "The whole computation of the transformation of each text file into a list of indices takes time and needs to be performed only once. The result is saved after the first time and then reloaded for all the other executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_train = np.zeros((n_files_trn, max_seq_len), dtype='int32')\n",
    "\n",
    "#file_counter = 0\n",
    "#for pf in pos_files_trn:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1\n",
    "\n",
    "#for nf in neg_files_trn:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_train[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_train[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "#np.save('idsMatrixTrain'+word_emb_size, ids_train)\n",
    "\n",
    "ids_train = np.load(ids_dir+'idsMatrixTrain'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids_test = np.zeros((n_files_test, max_seq_len), dtype='int32')\n",
    "\n",
    "#file_counter = 0\n",
    "#for pf in pos_files_test:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1\n",
    "\n",
    "#for nf in neg_files_test:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        idx_counter = 0\n",
    "#        line=f.readline()\n",
    "#        cleaned_line = clean_sentences(line)\n",
    "#        split = cleaned_line.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids_test[file_counter][idx_counter] = words_list.index(word)\n",
    "#            except ValueError:\n",
    "#                ids_test[file_counter][idx_counter] = 399999 #Vector for unkown words\n",
    "#            idx_counter = idx_counter + 1\n",
    "#            if idx_counter >= max_seq_len:\n",
    "#                break\n",
    "#        file_counter = file_counter + 1 \n",
    "\n",
    "#np.save('idsMatrixTest'+word_emb_size, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = np.load(ids_dir+'idsMatrixTest'+word_emb_size+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also create the **labels** with **one-hot-encoding** ([1, 0] for positive and [0, 1] for negative), as done in the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_pos_trn = len(pos_files_trn)\n",
    "len_neg_trn = len(neg_files_trn)\n",
    "y_train = [[1,0] for i in range(len_pos_trn)] + [[0,1] for i in range(len_neg_trn)]\n",
    "\n",
    "len_pos_test = len(pos_files_test)\n",
    "len_neg_test = len(neg_files_test)\n",
    "y_test = [[1,0] for i in range(len_pos_test)] + [[0,1] for i in range(len_neg_test)]\n",
    "\n",
    "len_trn = len_pos_trn + len_neg_trn\n",
    "len_test = len_pos_test + len_neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some premade estimators do not accept one-hot-encoding of the labels as explained here:https://stackoverflow.com/questions/48114258/tensorflow-estimator-number-of-classes-does-not-change\n",
    "\n",
    "For this reason I create a function which converts a list containing one-hot-incoded labels into a list containing **ordinal encoded** labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_2_ordinal(onehot):\n",
    "    n_classes = len(onehot[0])\n",
    "    ordinal = []\n",
    "    for i in range(len(onehot)):\n",
    "        for j in range(n_classes):\n",
    "            if onehot[i][j]==1:\n",
    "                ordinal.append(j)\n",
    "    return(ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ord = onehot_2_ordinal(y_train)\n",
    "y_test_ord = onehot_2_ordinal(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, I define a `nn.Module` model which does almost all the transformation we want, i.e. from the embedding of the indexes to the making of the logits. I copied some of the code I found in this tutorial:\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "Note that using `nn.CrossEntropyLoss()` as loss function is equivalent to combining `nn.LogSoftmax(dim=1)` as last layer of the model together with `nn.NLLLoss()` as loss function. I choose the first option here as it looks more similar to the implementation using TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeLSTM(nn.Module):\n",
    "    def __init__(self, weights_matrix, keep_prob, n_lstm_units):\n",
    "        super(GloVeLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(weights_matrix)\n",
    "        _, self.emb_dim = weights_matrix.shape\n",
    "        self.embeddings.weight.requires_grad=False\n",
    "        self.n_lstm_units = n_lstm_units\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.lstm = nn.LSTM(input_size=self.emb_dim,\n",
    "                            hidden_size=n_lstm_units,\n",
    "                            dropout=1-keep_prob).double()\n",
    "        self.hidden2bin = nn.Linear(n_lstm_units, 2).double()\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #Before using the LSTM, we need to clean\n",
    "        #its hidden and state variable in order\n",
    "        #to prevent the previous review to influence\n",
    "        #the output for the new review.\n",
    "        self.hidden = self.init_hidden()\n",
    "        emb_vect = self.embeddings(inp)\n",
    "        #We only care about the output of the LSTM for\n",
    "        #the last word of the sentence (which is the \n",
    "        #first entry of the self.hidden)\n",
    "        _, self.hidden = self.lstm(\n",
    "            emb_vect.view(len(inp), 1, -1), self.hidden)\n",
    "        logits = self.hidden2bin(self.hidden[0].view(1, -1))\n",
    "        return(logits)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        hidden = (torch.zeros(1, 1, self.n_lstm_units),\n",
    "                  torch.zeros(1, 1, self.n_lstm_units))\n",
    "        hidden = (torch.tensor(hidden[0], dtype=torch.float64),\n",
    "                  torch.tensor(hidden[1], dtype=torch.float64))\n",
    "        return(hidden)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, I make sure that this model works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "classifier = GloVeLSTM(weights_matrix=torch.from_numpy(word_vectors),\n",
    "                         keep_prob=0.7,\n",
    "                         n_lstm_units=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all input sequences which have a lot of zeros at the end, produce the same logits (without training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n",
      "[   116    285    998      7    530    439    413 399999  27015]\n",
      "tensor([[ 0.1573, -0.0554]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n",
      "[   13    37 15890     0     0     0     0     0     0]\n",
      "tensor([[ 0.1020, -0.0009]], dtype=torch.float64)\n",
      "[   0 3121    3    0 5317 1468 1351    0 8973]\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 8.4240, -9.8198]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "tensor([[ 0.1036,  0.0015]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(ids_train[i][-10:-1])\n",
    "    print(classifier(torch.from_numpy(np.int64(ids_train[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the **loss function**. From this Quora page: https://www.quora.com/What-are-the-differences-between-maximum-likelihood-and-cross-entropy-as-a-loss-function it seems that the negative log likelihood loss and the binary cross-entropy loss are the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimizer, one has to give only the parameters which have `requires_grad=True`, as explained here:\n",
    "https://discuss.pytorch.org/t/freeze-the-learnable-parameters-of-resnet-and-attach-it-to-a-new-network/949/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, classifier.parameters())\n",
    "optimizer = torch.optim.Adagrad(parameters, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try to train the model on a single input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.zero_grad()\n",
    "inp = torch.from_numpy(np.int64(ids_train[0]))\n",
    "y = y_train_ord[0:1]\n",
    "logits = classifier(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fcn(logits, torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6434, dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our custom Dataset. Note that there exists an official dataset for IMDB:\n",
    "\n",
    "https://torchtext.readthedocs.io/en/latest/datasets.html#imdb\n",
    "\n",
    "but I won't use it since my goal is to get comfortable with tools that I could later use for any set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIMDBDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ids_matrix, y_ord):\n",
    "        self.ids_matrix = ids_matrix\n",
    "        self.y_ord = y_ord\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(self.ids_matrix.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return((self.ids_matrix[idx], self.y_ord[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = MyIMDBDataset(ids_train, y_train_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, label = my_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Dataset` object outputs objects having the type that one would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12588,     14,     29, 399999,    319,     34,     20,     14,\n",
       "         2242,  31631,   2094,  11013,  17614,     22,     47,   1058,\n",
       "           14,     20,     14,      7, 399999,    319,     42, 255441,\n",
       "           30,    573,    100,   1299,    296,      0, 399999,  30748,\n",
       "            3,   2838,   4450,  13819,      0,    402,   4184,     17,\n",
       "            0,   4442,  14959,  10220,      5,  12588,  60607,  12256,\n",
       "            0, 399999,     34,   2909,  47119,    461,   4543,   1749,\n",
       "           25,      0,  12626,   3752,      5,   6801,      4,      0,\n",
       "         2037,   9742,   1174,     14,  34443,      5, 399999,      0,\n",
       "         3226,   4260,   1654,    107,    339,     77,    138,     22,\n",
       "           58,     34,     20,     14, 130086,   5610,      5,  16089,\n",
       "            0,  23277,   8652,     42,     14,   7206,    983,     37,\n",
       "          319,     42,   5635,    109,    615,     34,   1952,    306,\n",
       "        97532,   5319,   2909,  14014,      6,   2158,   5418,      3,\n",
       "            5,  12534,     10,  12588,      5,      0,    109,     38,\n",
       "          593,     71,   9919,  42692,      4,    155,     38,   1764,\n",
       "            4,    169,     20,    116,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when one use the `DataLoader` object, the objects that we want to iterate over are **automatically wrapped inside tensors**, as one can see in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[ 1.0000e+01,  3.5900e+02,  2.8100e+02,  ...,  7.5000e+01,\n",
      "          1.7000e+02,  1.2930e+03],\n",
      "        [ 4.1000e+01,  7.6321e+04,  6.4000e+01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 5.7720e+03,  3.8300e+03,  8.2200e+02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 4.0000e+05,  3.2755e+04,  5.0490e+03,  ...,  1.9870e+03,\n",
      "          1.0936e+04,  2.0400e+02],\n",
      "        [ 4.3770e+03,  9.4000e+01,  9.6500e+02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  1.6760e+03,  1.0019e+05,  ...,  0.0000e+00,\n",
      "          4.8810e+03,  3.0000e+00]], dtype=torch.int32)\n",
      "torch.Size([100, 250])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([ 1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  0,  1,  1,  0,\n",
      "         1,  1,  1,  0,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,\n",
      "         0,  1,  0,  1,  0,  0,  0,  1,  0,  0,  0,  1,  1,  0,\n",
      "         1,  0,  1,  0,  0,  1,  1,  1,  1,  1,  0,  0,  1,  1,\n",
      "         0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0,  0,\n",
      "         1,  0,  1,  0,  1,  1,  0,  0,  0,  1,  0,  0,  0,  1,\n",
      "         1,  1,  0,  0,  1,  1,  1,  0,  1,  1,  0,  0,  0,  1,\n",
      "         1,  1])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    if i>0:\n",
    "        break\n",
    "    print(i)\n",
    "    inp, label = data\n",
    "    print(type(inp))\n",
    "    print(inp)\n",
    "    print(inp.shape)\n",
    "    print(type(label))\n",
    "    print(label)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the following cell is a variation of what I found on this page: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 100, got 25000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-5c40dbf9366d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# put the inputs into a torch.tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-5964f1b9cf75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#first entry of the self.hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         _, self.hidden = self.lstm(\n\u001b[0;32m---> 25\u001b[0;31m             emb_vect.view(len(inp), 1, -1), self.hidden)\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    128\u001b[0m             raise RuntimeError(\n\u001b[1;32m    129\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 130\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_input_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 100, got 25000"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inp, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # put the inputs into a torch.tensor\n",
    "        inp = torch.from_numpy(np.int64(inp))\n",
    "        logits = classifier(inp)\n",
    "        loss = loss_fcn(logits, torch.tensor(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells we try to find sources of errors. For this we start by creating some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_toy = 0.7\n",
    "n_lstm_units_toy = 2\n",
    "len_vocab_toy = 14\n",
    "emb_size_toy = 3\n",
    "seq_len_toy = 4\n",
    "weights_matrix_toy = torch.from_numpy(np.random.random([len_vocab_toy, emb_size_toy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3443,  0.5233,  0.7177],\n",
      "        [ 0.1617,  0.0085,  0.9811],\n",
      "        [ 0.5504,  0.5442,  0.9117],\n",
      "        [ 0.7510,  0.9430,  0.0491],\n",
      "        [ 0.1687,  0.2143,  0.6186],\n",
      "        [ 0.3420,  0.7512,  0.0120],\n",
      "        [ 0.0893,  0.6593,  0.1254],\n",
      "        [ 0.4261,  0.4338,  0.8336],\n",
      "        [ 0.4347,  0.4546,  0.6529],\n",
      "        [ 0.9873,  0.6314,  0.8121],\n",
      "        [ 0.2329,  0.7288,  0.8390],\n",
      "        [ 0.1025,  0.4473,  0.9384],\n",
      "        [ 0.3217,  0.2832,  0.8762],\n",
      "        [ 0.5853,  0.7373,  0.2068]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(weights_matrix_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_toy = np.arange(12)\n",
    "ids_toy.shape = (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_toy = ids_toy.astype(np.int32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_toy = [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_toy = (torch.zeros(1, 1, n_lstm_units_toy),\n",
    "                torch.zeros(1, 1, n_lstm_units_toy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_toy = (torch.tensor(hidden_toy[0], dtype=torch.float64),\n",
    "              torch.tensor(hidden_toy[1], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_toy = nn.Embedding.from_pretrained(weights_matrix_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, emb_dim_toy = weights_matrix_toy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritz/anaconda3/envs/mypy36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "lstm_toy = nn.LSTM(input_size=emb_dim_toy,\n",
    "                            hidden_size=n_lstm_units_toy,\n",
    "                            dropout=1-keep_prob_toy).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden2bin_toy = nn.Linear(n_lstm_units_toy, 2).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logsoft_toy = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn_toy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try to reproduce what the `forward function` would do on our toy data, with this setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  5,  6,  7])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.from_numpy(np.int64(ids_toy[1]))\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vect_toy = embeddings_toy(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1687,  0.2143,  0.6186],\n",
       "        [ 0.3420,  0.7512,  0.0120],\n",
       "        [ 0.0893,  0.6593,  0.1254],\n",
       "        [ 0.4261,  0.4338,  0.8336]], dtype=torch.float64)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_vect_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1687,  0.2143,  0.6186],\n",
      "        [ 0.3420,  0.7512,  0.0120],\n",
      "        [ 0.0893,  0.6593,  0.1254],\n",
      "        [ 0.4261,  0.4338,  0.8336]])\n"
     ]
    }
   ],
   "source": [
    "#emb_vect_toy = torch.tensor(emb_vect_toy, dtype=torch.float32)\n",
    "print(emb_vect_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.3609, -0.1728]]], dtype=torch.float64), tensor([[[-0.4855, -0.3470]]], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "_, hidden_toy = lstm_toy(\n",
    "            emb_vect_toy.view(len(inp), 1, -1), hidden_toy)\n",
    "print(hidden_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3609, -0.1728]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_toy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_toy = hidden2bin_toy(hidden_toy[0].view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits_toy = logsoft_toy(logits_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7204, -0.1972]], dtype=torch.float64)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([y_toy[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7204, dtype=torch.float64)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fcn_toy(logits_toy, torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
